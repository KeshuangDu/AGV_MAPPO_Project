"""
å‚ç›´å¸ƒå±€ + å•å‘è·¯ç”± è¯„ä¼°è„šæœ¬
Vertical Layout + Unidirectional Routing Evaluation

è¿è¡Œæ–¹æ³•ï¼š
    python evaluate_v_uni.py --checkpoint ./data/checkpoints_v_uni/mappo_episode_100.pt --episodes 50
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import argparse
import json
from tqdm import tqdm

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.env_config import env_config
from config.train_config_v_uni import train_config
from config.model_config import model_config
from environment.port_env import PortEnvironment
from models.actor_critic import ActorCritic
from algorithm.mappo import MAPPO


class Evaluator:
    """è¯„ä¼°å™¨ç±»"""

    def __init__(self, checkpoint_path: str, experiment_name: str = 'v_uni'):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""

        self.experiment_name = experiment_name

        # âœ¨ è®¾ç½®ç¯å¢ƒä¸ºå‚ç›´+å•å‘æ¨¡å¼
        env_config.LAYOUT_TYPE = 'vertical'
        env_config.BIDIRECTIONAL = False

        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°å®éªŒ: å‚ç›´å¸ƒå±€ + å•å‘è·¯ç”± (v_uni)")
        print(f"{'='*60}")
        print(f"âš™ï¸  LAYOUT_TYPE = {env_config.LAYOUT_TYPE}")
        print(f"âš™ï¸  BIDIRECTIONAL = {env_config.BIDIRECTIONAL}")
        print(f"{'='*60}\n")

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = PortEnvironment(env_config)
        self.num_agents = env_config.NUM_AGVS

        # è®¡ç®—è§‚å¯Ÿç»´åº¦
        obs_dict, _ = self.env.reset()
        sample_obs = obs_dict['agent_0']
        self.obs_dim = (
            sample_obs['own_state'].shape[0] +
            sample_obs['nearby_agvs'].size +
            sample_obs['task_info'].shape[0] +
            sample_obs['path_occupancy'].shape[0]
        )

        # åˆå§‹åŒ–æ¨¡å‹
        self.actor_critic = ActorCritic(
            obs_dim=self.obs_dim,
            actor_hidden_dims=model_config.ACTOR_HIDDEN_DIMS,
            critic_hidden_dims=model_config.CRITIC_HIDDEN_DIMS,
            num_lanes=env_config.NUM_HORIZONTAL_LANES,
            num_directions=2,
            use_centralized_critic=model_config.USE_CENTRALIZED_CRITIC
        )

        # åˆå§‹åŒ–MAPPO
        self.mappo = MAPPO(
            actor_critic=self.actor_critic,
            num_agents=self.num_agents,
            device=self.device
        )

        # åŠ è½½æ¨¡å‹
        if os.path.exists(checkpoint_path):
            self.mappo.load(checkpoint_path)
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {checkpoint_path}")
        else:
            print(f"âŒ æ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹: {checkpoint_path}")
            sys.exit(1)

        # è¯„ä¼°ç»“æœï¼ˆå•å‘æ¨¡å¼ä¸è®°å½•åé€€ä½¿ç”¨ç‡ï¼‰
        self.eval_results = {
            'episode_rewards': [],
            'episode_lengths': [],
            'tasks_completed': [],
            'collisions': [],
            'direction_changes': []
        }

    def flatten_observation(self, obs_dict: dict) -> np.ndarray:
        """å±•å¹³è§‚å¯Ÿ"""
        obs_list = []
        obs_list.append(obs_dict['own_state'])
        obs_list.append(obs_dict['nearby_agvs'].flatten())
        obs_list.append(obs_dict['task_info'])
        obs_list.append(obs_dict['path_occupancy'])
        return np.concatenate(obs_list, axis=0)

    def evaluate_episode(self, render: bool = False) -> dict:
        """è¯„ä¼°å•ä¸ªepisode"""
        obs_dict, _ = self.env.reset()
        episode_reward = 0
        episode_length = 0
        done = False

        # è®°å½•æ–¹å‘å˜åŒ–
        direction_changes = [0] * self.num_agents
        prev_directions = [agv.moving_forward for agv in self.env.agvs]

        while not done:
            # æ”¶é›†è§‚å¯Ÿ
            obs_batch = []
            for i in range(self.num_agents):
                obs = self.flatten_observation(obs_dict[f'agent_{i}'])
                obs_batch.append(obs)

            obs_tensor = torch.FloatTensor(np.array(obs_batch)).to(self.device)

            # é€‰æ‹©åŠ¨ä½œ(ç¡®å®šæ€§)
            actions_tensor, _, _ = self.mappo.select_action(
                obs_tensor, deterministic=True
            )

            # âœ… ä¿®å¤ï¼šè½¬æ¢ä¸ºç¯å¢ƒæ ¼å¼ï¼Œä½¿ç”¨motion
            env_actions = {}
            for i in range(self.num_agents):
                env_actions[f'agent_{i}'] = {
                    'lane': actions_tensor['lane'][i].cpu().item(),
                    'direction': actions_tensor['direction'][i].cpu().item(),
                    'motion': actions_tensor['motion'][i].cpu().numpy()
                }

            # ç¯å¢ƒæ­¥è¿›
            next_obs_dict, rewards_dict, terminated, truncated, info = self.env.step(env_actions)
            done = terminated['__all__'] or truncated['__all__']

            # ç»Ÿè®¡æ–¹å‘å˜åŒ–
            for i in range(self.num_agents):
                curr_direction = self.env.agvs[i].moving_forward
                if curr_direction != prev_directions[i]:
                    direction_changes[i] += 1
                prev_directions[i] = curr_direction

            # ç´¯è®¡å¥–åŠ±
            rewards = np.array([rewards_dict[f'agent_{i}'] for i in range(self.num_agents)])
            episode_reward += rewards.mean()

            obs_dict = next_obs_dict
            episode_length += 1

            if render:
                self.env.render()

        return {
            'reward': episode_reward,
            'length': episode_length,
            'tasks_completed': info.get('tasks_completed', 0),
            'collisions': info.get('collisions', 0),
            'direction_changes': direction_changes
        }

    def evaluate(self, num_episodes: int = 50, render: bool = False):
        """è¿è¡Œè¯„ä¼°"""
        print(f"å¼€å§‹è¯„ä¼° - {num_episodes} episodes\n")

        for _ in tqdm(range(num_episodes), desc="Evaluating"):
            result = self.evaluate_episode(render=render)

            self.eval_results['episode_rewards'].append(result['reward'])
            self.eval_results['episode_lengths'].append(result['length'])
            self.eval_results['tasks_completed'].append(result['tasks_completed'])
            self.eval_results['collisions'].append(result['collisions'])
            self.eval_results['direction_changes'].append(np.mean(result['direction_changes']))

        self.print_results()
        self.save_results()
        self.plot_results()

    def print_results(self):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°ç»“æœç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"å¹³å‡å¥–åŠ±: {np.mean(self.eval_results['episode_rewards']):.2f}")
        print(f"å¹³å‡æ­¥æ•°: {np.mean(self.eval_results['episode_lengths']):.2f}")
        print(f"å¹³å‡ä»»åŠ¡å®Œæˆæ•°: {np.mean(self.eval_results['tasks_completed']):.2f}")
        print(f"å¹³å‡ç¢°æ’æ¬¡æ•°: {np.mean(self.eval_results['collisions']):.2f}")
        print(f"å¹³å‡æ–¹å‘å˜åŒ–: {np.mean(self.eval_results['direction_changes']):.2f}æ¬¡")
        print(f"{'='*60}")

    def save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        results_dir = f'./data/eval_results_{self.experiment_name}'
        os.makedirs(results_dir, exist_ok=True)

        results_path = os.path.join(results_dir, 'eval_results.json')
        with open(results_path, 'w') as f:
            json.dump({
                'mean_reward': float(np.mean(self.eval_results['episode_rewards'])),
                'std_reward': float(np.std(self.eval_results['episode_rewards'])),
                'mean_length': float(np.mean(self.eval_results['episode_lengths'])),
                'mean_tasks': float(np.mean(self.eval_results['tasks_completed'])),
                'mean_collisions': float(np.mean(self.eval_results['collisions'])),
                'mean_direction_changes': float(np.mean(self.eval_results['direction_changes'])),
                'all_results': {k: [float(x) for x in v] for k, v in self.eval_results.items()}
            }, f, indent=2)

        print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    def plot_results(self):
        """ç»˜åˆ¶è¯„ä¼°ç»“æœ"""
        results_dir = f'./data/eval_results_{self.experiment_name}'
        os.makedirs(results_dir, exist_ok=True)

        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # å¥–åŠ±åˆ†å¸ƒ
        axes[0, 0].hist(self.eval_results['episode_rewards'], bins=20, edgecolor='black')
        axes[0, 0].set_title('Episode Rewards Distribution')
        axes[0, 0].set_xlabel('Reward')
        axes[0, 0].set_ylabel('Frequency')

        # ä»»åŠ¡å®Œæˆæ•°
        axes[0, 1].hist(self.eval_results['tasks_completed'], bins=20, edgecolor='black')
        axes[0, 1].set_title('Tasks Completed Distribution')
        axes[0, 1].set_xlabel('Tasks')
        axes[0, 1].set_ylabel('Frequency')

        # ç¢°æ’æ¬¡æ•°
        axes[1, 0].hist(self.eval_results['collisions'], bins=20, edgecolor='black')
        axes[1, 0].set_title('Collisions Distribution')
        axes[1, 0].set_xlabel('Collisions')
        axes[1, 0].set_ylabel('Frequency')

        # Episodeé•¿åº¦
        axes[1, 1].hist(self.eval_results['episode_lengths'], bins=20, edgecolor='black')
        axes[1, 1].set_title('Episode Length Distribution')
        axes[1, 1].set_xlabel('Steps')
        axes[1, 1].set_ylabel('Frequency')

        plt.tight_layout()
        plot_path = os.path.join(results_dir, 'eval_results.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
        plt.close()


def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°å‚ç›´å¸ƒå±€+å•å‘è·¯ç”±æ¨¡å‹')
    parser.add_argument('--checkpoint', type=str, required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=50, help='è¯„ä¼°è½®æ•°')
    parser.add_argument('--render', action='store_true', help='æ˜¯å¦æ¸²æŸ“')

    args = parser.parse_args()

    evaluator = Evaluator(checkpoint_path=args.checkpoint, experiment_name='v_uni')
    evaluator.evaluate(num_episodes=args.episodes, render=args.render)


if __name__ == "__main__":
    main()