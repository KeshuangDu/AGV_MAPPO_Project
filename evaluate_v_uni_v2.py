"""
å‚ç›´å¸ƒå±€ + å•å‘è·¯ç”± è¯„ä¼°è„šæœ¬ v2.0
Vertical Layout + Unidirectional Routing Evaluation with Task Time Tracking

æ–°å¢åŠŸèƒ½ï¼š
- â±ï¸ ä»»åŠ¡å®Œæˆæ—¶é—´è¿½è¸ª
- ğŸ“Š æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
- ğŸ“ˆ æ—¶é—´ç›¸å…³å¯è§†åŒ–

è¿è¡Œæ–¹æ³•ï¼š
    python evaluate_v_uni_v2.py --checkpoint ./data/checkpoints_v_uni/mappo_episode_100.pt --episodes 50
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import argparse
import json
from tqdm import tqdm

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.env_config import env_config
from config.train_config_v_uni import train_config
from config.model_config import model_config
from environment.port_env import PortEnvironment
from models.actor_critic import ActorCritic
from algorithm.mappo import MAPPO


class EvaluatorV2:
    """è¯„ä¼°å™¨ç±» v2.0 - å¸¦ä»»åŠ¡æ—¶é—´è¿½è¸ª"""

    def __init__(self, checkpoint_path: str, experiment_name: str = 'v_uni'):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""

        self.experiment_name = experiment_name

        # âœ¨ è®¾ç½®ç¯å¢ƒä¸ºå‚ç›´+å•å‘æ¨¡å¼
        env_config.LAYOUT_TYPE = 'vertical'
        env_config.BIDIRECTIONAL = False

        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°å®éªŒ v2.0: å‚ç›´å¸ƒå±€ + å•å‘è·¯ç”± (v_uni)")
        print(f"âœ¨ æ–°å¢: ä»»åŠ¡å®Œæˆæ—¶é—´è¿½è¸ª")
        print(f"{'='*60}")
        print(f"âš™ï¸  LAYOUT_TYPE = {env_config.LAYOUT_TYPE}")
        print(f"âš™ï¸  BIDIRECTIONAL = {env_config.BIDIRECTIONAL}")
        print(f"{'='*60}\n")

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = PortEnvironment(env_config)
        self.num_agents = env_config.NUM_AGVS

        # è®¡ç®—è§‚å¯Ÿç»´åº¦
        obs_dict, _ = self.env.reset()
        sample_obs = obs_dict['agent_0']
        self.obs_dim = (
            sample_obs['own_state'].shape[0] +
            sample_obs['nearby_agvs'].size +
            sample_obs['task_info'].shape[0] +
            sample_obs['path_occupancy'].shape[0]
        )

        # åˆå§‹åŒ–æ¨¡å‹
        self.actor_critic = ActorCritic(
            obs_dim=self.obs_dim,
            actor_hidden_dims=model_config.ACTOR_HIDDEN_DIMS,
            critic_hidden_dims=model_config.CRITIC_HIDDEN_DIMS,
            num_lanes=env_config.NUM_HORIZONTAL_LANES,
            num_directions=2,
            use_centralized_critic=model_config.USE_CENTRALIZED_CRITIC
        )

        # åˆå§‹åŒ–MAPPO
        self.mappo = MAPPO(
            actor_critic=self.actor_critic,
            num_agents=self.num_agents,
            device=self.device
        )

        # åŠ è½½æ¨¡å‹
        if os.path.exists(checkpoint_path):
            self.mappo.load(checkpoint_path)
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {checkpoint_path}")
        else:
            print(f"âŒ æ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹: {checkpoint_path}")
            sys.exit(1)

        # è¯„ä¼°ç»“æœ
        self.eval_results = {
            'episode_rewards': [],
            'episode_lengths': [],
            'tasks_completed': [],
            'collisions': [],
            'direction_changes': [],
            'task_completion_times': []  # âœ¨ æ–°å¢ï¼šä»»åŠ¡å®Œæˆæ—¶é—´åˆ—è¡¨
        }

    def flatten_observation(self, obs_dict: dict) -> np.ndarray:
        """å±•å¹³è§‚å¯Ÿ"""
        obs_list = []
        obs_list.append(obs_dict['own_state'])
        obs_list.append(obs_dict['nearby_agvs'].flatten())
        obs_list.append(obs_dict['task_info'])
        obs_list.append(obs_dict['path_occupancy'])
        return np.concatenate(obs_list, axis=0)

    def evaluate_episode(self, render: bool = False) -> dict:
        """è¯„ä¼°å•ä¸ªepisode"""
        obs_dict, _ = self.env.reset()
        episode_reward = 0
        episode_length = 0
        done = False

        # è®°å½•æ–¹å‘å˜åŒ–
        direction_changes = [0] * self.num_agents
        prev_directions = [agv.moving_forward for agv in self.env.agvs]

        # âœ¨ æ”¶é›†æœ¬episodeçš„ä»»åŠ¡å®Œæˆæ—¶é—´
        episode_task_times = []

        while not done:
            # æ”¶é›†è§‚å¯Ÿ
            obs_batch = []
            for i in range(self.num_agents):
                obs = self.flatten_observation(obs_dict[f'agent_{i}'])
                obs_batch.append(obs)

            obs_tensor = torch.FloatTensor(np.array(obs_batch)).to(self.device)

            # é€‰æ‹©åŠ¨ä½œ(ç¡®å®šæ€§)
            actions_tensor, _, _ = self.mappo.select_action(
                obs_tensor, deterministic=True
            )

            # âœ… ä¿®å¤ï¼šè½¬æ¢ä¸ºç¯å¢ƒæ ¼å¼ï¼Œä½¿ç”¨motion
            env_actions = {}
            for i in range(self.num_agents):
                env_actions[f'agent_{i}'] = {
                    'lane': actions_tensor['lane'][i].cpu().item(),
                    'direction': actions_tensor['direction'][i].cpu().item(),
                    'motion': actions_tensor['motion'][i].cpu().numpy()
                }

            # ç¯å¢ƒæ­¥è¿›
            next_obs_dict, rewards_dict, terminated, truncated, info = self.env.step(env_actions)
            done = terminated['__all__'] or truncated['__all__']

            # ç»Ÿè®¡æ–¹å‘å˜åŒ–
            for i in range(self.num_agents):
                curr_direction = self.env.agvs[i].moving_forward
                if curr_direction != prev_directions[i]:
                    direction_changes[i] += 1
                prev_directions[i] = curr_direction

            # âœ¨ æ”¶é›†æœ¬æ­¥å®Œæˆçš„ä»»åŠ¡æ—¶é—´
            if 'completed_tasks_this_step' in info and info['completed_tasks_this_step']:
                for task in info['completed_tasks_this_step']:
                    if hasattr(task, 'completion_time') and task.completion_time is not None:
                        episode_task_times.append(task.completion_time)

            # ç´¯è®¡å¥–åŠ±
            rewards = np.array([rewards_dict[f'agent_{i}'] for i in range(self.num_agents)])
            episode_reward += rewards.mean()

            obs_dict = next_obs_dict
            episode_length += 1

            if render:
                self.env.render()

        return {
            'reward': episode_reward,
            'length': episode_length,
            'tasks_completed': info.get('tasks_completed', 0),
            'collisions': info.get('collisions', 0),
            'direction_changes': direction_changes,
            'task_times': episode_task_times  # âœ¨ è¿”å›ä»»åŠ¡æ—¶é—´
        }

    def evaluate(self, num_episodes: int = 50, render: bool = False):
        """è¿è¡Œè¯„ä¼°"""
        print(f"å¼€å§‹è¯„ä¼° - {num_episodes} episodes\n")

        for _ in tqdm(range(num_episodes), desc="Evaluating"):
            result = self.evaluate_episode(render=render)

            self.eval_results['episode_rewards'].append(result['reward'])
            self.eval_results['episode_lengths'].append(result['length'])
            self.eval_results['tasks_completed'].append(result['tasks_completed'])
            self.eval_results['collisions'].append(result['collisions'])
            self.eval_results['direction_changes'].append(np.mean(result['direction_changes']))

            # âœ¨ æ”¶é›†æ‰€æœ‰ä»»åŠ¡å®Œæˆæ—¶é—´
            self.eval_results['task_completion_times'].extend(result['task_times'])

        self.print_results()
        self.save_results()
        self.plot_results()

    def print_results(self):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°ç»“æœç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"å¹³å‡å¥–åŠ±: {np.mean(self.eval_results['episode_rewards']):.2f}")
        print(f"å¹³å‡æ­¥æ•°: {np.mean(self.eval_results['episode_lengths']):.2f}")
        print(f"å¹³å‡ä»»åŠ¡å®Œæˆæ•°: {np.mean(self.eval_results['tasks_completed']):.2f}")
        print(f"å¹³å‡ç¢°æ’æ¬¡æ•°: {np.mean(self.eval_results['collisions']):.2f}")
        print(f"å¹³å‡æ–¹å‘å˜åŒ–: {np.mean(self.eval_results['direction_changes']):.2f}æ¬¡")

        # âœ¨ ä»»åŠ¡æ—¶é—´ç»Ÿè®¡
        if self.eval_results['task_completion_times']:
            times = self.eval_results['task_completion_times']
            print(f"\nâœ¨ ä»»åŠ¡å®Œæˆæ—¶é—´ç»Ÿè®¡:")
            print(f"  - æ€»å®Œæˆä»»åŠ¡æ•°: {len(times)}")
            print(f"  - å¹³å‡å®Œæˆæ—¶é—´: {np.mean(times):.2f}ç§’")
            print(f"  - æœ€çŸ­å®Œæˆæ—¶é—´: {np.min(times):.2f}ç§’")
            print(f"  - æœ€é•¿å®Œæˆæ—¶é—´: {np.max(times):.2f}ç§’")
            print(f"  - æ ‡å‡†å·®: {np.std(times):.2f}ç§’")
        else:
            print(f"\nâš ï¸  è­¦å‘Š: æ²¡æœ‰ä»»åŠ¡è¢«å®Œæˆ")

        print(f"{'='*60}")

    def save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        results_dir = f'./data/eval_results_{self.experiment_name}_v2'
        os.makedirs(results_dir, exist_ok=True)

        results_data = {
            'mean_reward': float(np.mean(self.eval_results['episode_rewards'])),
            'std_reward': float(np.std(self.eval_results['episode_rewards'])),
            'mean_length': float(np.mean(self.eval_results['episode_lengths'])),
            'mean_tasks': float(np.mean(self.eval_results['tasks_completed'])),
            'mean_collisions': float(np.mean(self.eval_results['collisions'])),
            'mean_direction_changes': float(np.mean(self.eval_results['direction_changes'])),
        }

        # âœ¨ æ·»åŠ ä»»åŠ¡æ—¶é—´ç»Ÿè®¡
        if self.eval_results['task_completion_times']:
            times = self.eval_results['task_completion_times']
            results_data.update({
                'total_tasks_completed': len(times),
                'mean_task_time': float(np.mean(times)),
                'min_task_time': float(np.min(times)),
                'max_task_time': float(np.max(times)),
                'std_task_time': float(np.std(times)),
                'all_task_times': [float(t) for t in times]
            })

        # âœ… ä¿®å¤ï¼šå¤„ç†ç©ºåˆ—è¡¨çš„æƒ…å†µ
        results_data['all_results'] = {}
        for k, v in self.eval_results.items():
            if len(v) > 0:
                if isinstance(v[0], (int, float, np.number)):
                    results_data['all_results'][k] = [float(x) for x in v]
                else:
                    results_data['all_results'][k] = v
            else:
                # ç©ºåˆ—è¡¨ç›´æ¥ä¿å­˜
                results_data['all_results'][k] = []

        results_path = os.path.join(results_dir, 'eval_results_v2.json')
        with open(results_path, 'w') as f:
            json.dump(results_data, f, indent=2)

        print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    def plot_results(self):
        """ç»˜åˆ¶è¯„ä¼°ç»“æœ"""
        results_dir = f'./data/eval_results_{self.experiment_name}_v2'
        os.makedirs(results_dir, exist_ok=True)

        # âœ¨ å¦‚æœæœ‰ä»»åŠ¡æ—¶é—´æ•°æ®ï¼Œç»˜åˆ¶5ä¸ªå›¾
        if self.eval_results['task_completion_times']:
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            axes = axes.flatten()
        else:
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            axes = axes.flatten()

        # å¥–åŠ±åˆ†å¸ƒ
        axes[0].hist(self.eval_results['episode_rewards'], bins=20, edgecolor='black')
        axes[0].set_title('Episode Rewards Distribution')
        axes[0].set_xlabel('Reward')
        axes[0].set_ylabel('Frequency')

        # ä»»åŠ¡å®Œæˆæ•°
        axes[1].hist(self.eval_results['tasks_completed'], bins=20, edgecolor='black')
        axes[1].set_title('Tasks Completed Distribution')
        axes[1].set_xlabel('Tasks')
        axes[1].set_ylabel('Frequency')

        # ç¢°æ’æ¬¡æ•°
        axes[2].hist(self.eval_results['collisions'], bins=20, edgecolor='black')
        axes[2].set_title('Collisions Distribution')
        axes[2].set_xlabel('Collisions')
        axes[2].set_ylabel('Frequency')

        # Episodeé•¿åº¦
        axes[3].hist(self.eval_results['episode_lengths'], bins=20, edgecolor='black')
        axes[3].set_title('Episode Length Distribution')
        axes[3].set_xlabel('Steps')
        axes[3].set_ylabel('Frequency')

        # âœ¨ ä»»åŠ¡å®Œæˆæ—¶é—´åˆ†å¸ƒ
        if self.eval_results['task_completion_times']:
            axes[4].hist(self.eval_results['task_completion_times'], bins=20,
                        edgecolor='black', color='green', alpha=0.7)
            axes[4].set_title('Task Completion Time Distribution')
            axes[4].set_xlabel('Time (seconds)')
            axes[4].set_ylabel('Frequency')
            axes[4].axvline(np.mean(self.eval_results['task_completion_times']),
                           color='red', linestyle='--', label='Mean')
            axes[4].legend()

            # åˆ é™¤ç¬¬6ä¸ªå­å›¾ï¼ˆå¦‚æœæœ‰ï¼‰
            if len(axes) > 5:
                fig.delaxes(axes[5])

        plt.tight_layout()
        plot_path = os.path.join(results_dir, 'eval_results_v2.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
        plt.close()


def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°å‚ç›´å¸ƒå±€+å•å‘è·¯ç”±æ¨¡å‹ (v2)')
    parser.add_argument('--checkpoint', type=str, required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=50, help='è¯„ä¼°è½®æ•°')
    parser.add_argument('--render', action='store_true', help='æ˜¯å¦æ¸²æŸ“')

    args = parser.parse_args()

    evaluator = EvaluatorV2(checkpoint_path=args.checkpoint, experiment_name='v_uni')
    evaluator.evaluate(num_episodes=args.episodes, render=args.render)


if __name__ == "__main__":
    main()