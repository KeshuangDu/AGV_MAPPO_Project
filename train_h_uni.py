"""
å®éªŒ2ï¼šæ°´å¹³å¸ƒå±€ + å•å‘è·¯ç”± è®­ç»ƒè„šæœ¬
Horizontal Layout + Unidirectional Routing

è¿è¡Œæ–¹æ³•ï¼š
    python train_h_uni.py

TensorBoardç›‘æ§ï¼š
    tensorboard --logdir=./runs_h_uni

âœ¨ å…³é”®å·®å¼‚ï¼š
- ä½¿ç”¨ train_config_h_uni é…ç½®
- è®¾ç½® BIDIRECTIONAL = False
- å…¶ä½™ä»£ç ä¸ train_h_bi.py å®Œå…¨ç›¸åŒ
"""

import os
import sys
import torch
import numpy as np
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import json
from datetime import datetime
import pickle

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.env_config import env_config
from config.train_config_h_uni import train_config  # âœ¨ ä½¿ç”¨æ°´å¹³å•å‘é…ç½®
from config.model_config import model_config
from environment.port_env import PortEnvironment
from models.actor_critic import ActorCritic
from algorithm.mappo import MAPPO


class Trainer:
    """è®­ç»ƒå™¨ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""

        # âœ¨ è®¾ç½®ç¯å¢ƒä¸ºå•å‘æ¨¡å¼
        env_config.BIDIRECTIONAL = False  # âœ¨âœ¨ å…³é”®ï¼
        print(f"\n{'='*60}")
        print(f"ğŸš€ å®éªŒ2ï¼šæ°´å¹³å¸ƒå±€ + å•å‘è·¯ç”± (h_uni)")  # âœ¨ å·®å¼‚3
        print(f"{'='*60}")
        print(f"âš ï¸  BIDIRECTIONAL = {env_config.BIDIRECTIONAL}")
        print(f"âš ï¸  AGVåªèƒ½å‰è¿›ï¼Œä¸èƒ½åé€€ï¼")  # âœ¨ å·®å¼‚4
        print(f"{'='*60}\n")

        # è®¾ç½®éšæœºç§å­
        self.set_seed(train_config.SEED)

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(
            'cuda' if torch.cuda.is_available() and train_config.USE_CUDA
            else 'cpu'
        )
        print(f"Using device: {self.device}")

        # åˆ›å»ºç›®å½•
        self.create_directories()

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = PortEnvironment(env_config)
        self.num_agents = env_config.NUM_AGVS

        # è®¡ç®—è§‚å¯Ÿç»´åº¦
        self.obs_dim = self.calculate_obs_dim()

        # åˆå§‹åŒ–æ¨¡å‹
        self.actor_critic = ActorCritic(
            obs_dim=self.obs_dim,
            actor_hidden_dims=model_config.ACTOR_HIDDEN_DIMS,
            critic_hidden_dims=model_config.CRITIC_HIDDEN_DIMS,
            num_lanes=env_config.NUM_HORIZONTAL_LANES,
            num_directions=2,
            use_centralized_critic=model_config.USE_CENTRALIZED_CRITIC
        )

        # åˆå§‹åŒ–MAPPOç®—æ³•
        self.mappo = MAPPO(
            actor_critic=self.actor_critic,
            num_agents=self.num_agents,
            lr_actor=train_config.ACTOR_LR,
            lr_critic=train_config.CRITIC_LR,
            gamma=train_config.GAMMA,
            gae_lambda=train_config.GAE_LAMBDA,
            clip_epsilon=train_config.CLIP_EPSILON,
            value_loss_coef=train_config.VALUE_LOSS_COEF,
            entropy_coef=train_config.ENTROPY_COEF,
            max_grad_norm=train_config.MAX_GRAD_NORM,
            ppo_epochs=train_config.PPO_EPOCHS,
            device=self.device
        )

        # TensorBoard
        if train_config.USE_TENSORBOARD:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_dir = os.path.join(train_config.TB_LOG_DIR, f"run_{timestamp}")
            self.writer = SummaryWriter(log_dir)
            print(f"TensorBoard logs: {log_dir}")
            print(f"å¯åŠ¨å‘½ä»¤: tensorboard --logdir={train_config.TB_LOG_DIR}")
        else:
            self.writer = None

        # è®­ç»ƒç»Ÿè®¡
        self.total_steps = 0
        self.episode_rewards = []
        self.episode_tasks_completed = []
        self.training_data = []

    def set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)

    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        os.makedirs(train_config.CHECKPOINT_DIR, exist_ok=True)
        os.makedirs(train_config.LOG_DIR, exist_ok=True)
        os.makedirs(train_config.DATA_DIR, exist_ok=True)
        os.makedirs(train_config.TB_LOG_DIR, exist_ok=True)

    def calculate_obs_dim(self) -> int:
        """è®¡ç®—è§‚å¯Ÿç»´åº¦"""
        obs_dim = 7 + 5 * 4 + 6 + env_config.NUM_HORIZONTAL_LANES
        return obs_dim

    def flatten_observation(self, obs_dict: dict) -> np.ndarray:
        """å±•å¹³è§‚å¯Ÿ"""
        obs_list = []
        obs_list.append(obs_dict['own_state'])
        obs_list.append(obs_dict['nearby_agvs'].flatten())
        obs_list.append(obs_dict['task_info'])
        obs_list.append(obs_dict['path_occupancy'])
        return np.concatenate(obs_list, axis=0)

    def collect_rollout(self) -> dict:
        """æ”¶é›†ä¸€ä¸ªepisodeçš„ç»éªŒ"""
        # åˆå§‹åŒ–ç¼“å†²åŒº
        observations = []
        actions = {'lane': [], 'direction': [], 'motion': []}
        log_probs = []
        rewards = []
        values = []
        dones = []

        # é‡ç½®ç¯å¢ƒ
        obs_dict, info = self.env.reset()

        episode_data = {
            'initial_state': {
                'agv_positions': [agv.position.tolist() for agv in self.env.agvs],
                'tasks': [task.get_info() for task in self.env.tasks]
            },
            'transitions': []
        }

        episode_reward = 0
        tasks_completed = 0

        for step in range(train_config.MAX_STEPS_PER_EPISODE):
            # æ”¶é›†æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚å¯Ÿ
            obs_batch = []
            for i in range(self.num_agents):
                obs = self.flatten_observation(obs_dict[f'agent_{i}'])
                obs_batch.append(obs)

            obs_tensor = torch.FloatTensor(np.array(obs_batch)).to(self.device)

            # é€‰æ‹©åŠ¨ä½œ
            actions_tensor, log_probs_tensor, values_tensor = self.mappo.select_action(
                obs_tensor
            )

            # è½¬æ¢ä¸ºç¯å¢ƒæ ¼å¼
            env_actions = {}
            for i in range(self.num_agents):
                env_actions[f'agent_{i}'] = {
                    'lane': actions_tensor['lane'][i].cpu().item(),
                    'direction': actions_tensor['direction'][i].cpu().item(),
                    'motion': actions_tensor['motion'][i].cpu().numpy()
                }

            # ç¯å¢ƒæ­¥è¿›
            next_obs_dict, reward_dict, terminated_dict, truncated_dict, info_dict = self.env.step(env_actions)

            # æ”¶é›†å¥–åŠ±
            reward_batch = np.array([
                reward_dict[f'agent_{i}'] for i in range(self.num_agents)
            ])

            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            done = terminated_dict['__all__'] or truncated_dict['__all__']
            done_batch = np.array([done] * self.num_agents)

            # å­˜å‚¨ç»éªŒ
            observations.append(obs_tensor.cpu())
            actions['lane'].append(actions_tensor['lane'].cpu())
            actions['direction'].append(actions_tensor['direction'].cpu())
            actions['motion'].append(actions_tensor['motion'].cpu())
            log_probs.append(log_probs_tensor.cpu())
            rewards.append(torch.FloatTensor(reward_batch))
            values.append(values_tensor.cpu())
            dones.append(torch.FloatTensor(done_batch))

            episode_data['transitions'].append({
                'step': step,
                'observations': obs_batch,
                'actions': {k: v.cpu().numpy().tolist() for k, v in actions_tensor.items()},
                'rewards': reward_batch.tolist(),
                'info': info_dict.get('agent_0', {})
            })

            episode_reward += reward_batch.mean()
            tasks_completed = info_dict.get('agent_0', {}).get('completed_tasks', 0)

            obs_dict = next_obs_dict

            if done:
                break

        # è®¡ç®—æœ€åçš„value
        next_obs_batch = []
        for i in range(self.num_agents):
            obs = self.flatten_observation(obs_dict[f'agent_{i}'])
            next_obs_batch.append(obs)
        next_obs_tensor = torch.FloatTensor(np.array(next_obs_batch)).to(self.device)

        with torch.no_grad():
            next_values = self.mappo.actor_critic.get_value(next_obs_tensor).cpu()

        # è½¬æ¢ä¸ºtensor
        observations = torch.stack(observations)
        log_probs = torch.stack(log_probs)
        rewards = torch.stack(rewards)
        values = torch.stack(values).squeeze(-1)
        dones = torch.stack(dones)

        # è®¡ç®—GAE
        advantages, returns = self.mappo.compute_gae(
            rewards, values, dones, next_values.squeeze(-1)
        )

        # æ„å»ºrollout buffer
        rollout_buffer = {
            'observations': observations.reshape(-1, self.obs_dim),
            'actions': {
                'lane': torch.stack(actions['lane']).reshape(-1),
                'direction': torch.stack(actions['direction']).reshape(-1),
                'motion': torch.stack(actions['motion']).reshape(-1, 2)
            },
            'log_probs': log_probs.reshape(-1),
            'advantages': advantages.reshape(-1),
            'returns': returns.reshape(-1)
        }

        episode_data['episode_reward'] = episode_reward
        episode_data['episode_length'] = step + 1
        episode_data['tasks_completed'] = tasks_completed
        self.training_data.append(episode_data)

        return rollout_buffer, episode_reward, tasks_completed

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\n" + "=" * 60)
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼šæ°´å¹³å¸ƒå±€ + åŒå‘è·¯ç”±")
        print("=" * 60)
        print(f"ç¯å¢ƒ: æ°´å¹³å¸ƒå±€æ¸¯å£ + åŒå‘è·¯ç”±")
        print(f"AGVæ•°é‡: {self.num_agents}")
        print(f"è®­ç»ƒè½®æ•°: {train_config.NUM_EPISODES}")
        print(f"æ¯è½®æœ€å¤§æ­¥æ•°: {train_config.MAX_STEPS_PER_EPISODE}")
        print(f"å¥–åŠ±ç±»å‹: {env_config.REWARD_TYPE}")
        print(f"ä»»åŠ¡ç®¡ç†å™¨: {'å¯ç”¨' if env_config.USE_TASK_MANAGER else 'ç¦ç”¨'}")
        print("=" * 60)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ç›®å½•: {train_config.CHECKPOINT_DIR}")
        print(f"ğŸ“Š TensorBoard: tensorboard --logdir={train_config.TB_LOG_DIR}")
        print("=" * 60 + "\n")

        best_avg_tasks = 0

        for episode in tqdm(range(train_config.NUM_EPISODES), desc="Training"):
            # æ”¶é›†ç»éªŒ
            rollout_buffer, episode_reward, tasks_completed = self.collect_rollout()

            # æ›´æ–°ç­–ç•¥
            metrics = self.mappo.update(rollout_buffer)

            # è®°å½•ç»Ÿè®¡
            self.episode_rewards.append(episode_reward)
            self.episode_tasks_completed.append(tasks_completed)

            # æ—¥å¿—è®°å½•
            if (episode + 1) % train_config.LOG_INTERVAL == 0:
                avg_reward = np.mean(self.episode_rewards[-train_config.LOG_INTERVAL:])
                avg_tasks = np.mean(self.episode_tasks_completed[-train_config.LOG_INTERVAL:])

                print(f"\nğŸ“ˆ Episode {episode + 1}/{train_config.NUM_EPISODES}")
                print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
                print(f"  å¹³å‡ä»»åŠ¡å®Œæˆæ•°: {avg_tasks:.2f}")
                print(f"  Actor Loss: {metrics['actor_loss']:.4f}")
                print(f"  Value Loss: {metrics['value_loss']:.4f}")
                print(f"  Entropy: {metrics['entropy']:.4f}")

                # TensorBoard
                if self.writer:
                    self.writer.add_scalar('Train/EpisodeReward', episode_reward, episode)
                    self.writer.add_scalar('Train/AvgReward', avg_reward, episode)
                    self.writer.add_scalar('Train/TasksCompleted', tasks_completed, episode)
                    self.writer.add_scalar('Train/AvgTasksCompleted', avg_tasks, episode)
                    self.writer.add_scalar('Train/ActorLoss', metrics['actor_loss'], episode)
                    self.writer.add_scalar('Train/ValueLoss', metrics['value_loss'], episode)
                    self.writer.add_scalar('Train/Entropy', metrics['entropy'], episode)

                # å¦‚æœä»»åŠ¡å®Œæˆæ•°åˆ›æ–°é«˜ï¼Œé¢å¤–ä¿å­˜
                if avg_tasks > best_avg_tasks:
                    best_avg_tasks = avg_tasks
                    best_checkpoint_path = os.path.join(
                        train_config.CHECKPOINT_DIR,
                        f"mappo_best_tasks_{avg_tasks:.1f}.pt"
                    )
                    self.mappo.save(best_checkpoint_path)
                    print(f"  ğŸŒŸ æ–°çºªå½•ï¼ä¿å­˜æœ€ä½³æ¨¡å‹: {best_checkpoint_path}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (episode + 1) % train_config.SAVE_INTERVAL == 0:
                checkpoint_path = os.path.join(
                    train_config.CHECKPOINT_DIR,
                    f"mappo_episode_{episode + 1}.pt"
                )
                self.mappo.save(checkpoint_path)

                # ä¿å­˜è®­ç»ƒæ•°æ®
                data_path = os.path.join(
                    train_config.DATA_DIR,
                    f"training_data_episode_{episode + 1}.pkl"
                )
                with open(data_path, 'wb') as f:
                    pickle.dump(self.training_data[-train_config.SAVE_INTERVAL:], f)

            # è¯„ä¼°
            if (episode + 1) % train_config.EVAL_INTERVAL == 0:
                eval_reward, eval_tasks = self.evaluate()
                print(f"  ğŸ“Š è¯„ä¼° - å¥–åŠ±: {eval_reward:.2f}, ä»»åŠ¡: {eval_tasks:.2f}")

                if self.writer:
                    self.writer.add_scalar('Eval/Reward', eval_reward, episode)
                    self.writer.add_scalar('Eval/TasksCompleted', eval_tasks, episode)

        # è®­ç»ƒç»“æŸ
        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"æœ€ä½³å¹³å‡ä»»åŠ¡å®Œæˆæ•°: {best_avg_tasks:.2f}")
        print("=" * 60)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(
            train_config.CHECKPOINT_DIR,
            f"mappo_final_{train_config.NUM_EPISODES}ep.pt"
        )
        self.mappo.save(final_model_path)

        # ä¿å­˜æ‰€æœ‰è®­ç»ƒæ•°æ®
        all_data_path = os.path.join(
            train_config.DATA_DIR,
            "all_training_data.pkl"
        )
        with open(all_data_path, 'wb') as f:
            pickle.dump(self.training_data, f)

        # ä¿å­˜è®­ç»ƒæ›²çº¿
        self.save_training_curves()

        if self.writer:
            self.writer.close()

    def evaluate(self, num_episodes: int = None) -> tuple:
        """è¯„ä¼°ç­–ç•¥"""
        if num_episodes is None:
            num_episodes = train_config.NUM_EVAL_EPISODES

        eval_rewards = []
        eval_tasks = []

        for _ in range(num_episodes):
            obs_dict, _ = self.env.reset()
            episode_reward = 0
            done = False

            while not done:
                obs_batch = []
                for i in range(self.num_agents):
                    obs = self.flatten_observation(obs_dict[f'agent_{i}'])
                    obs_batch.append(obs)

                obs_tensor = torch.FloatTensor(np.array(obs_batch)).to(self.device)

                actions_tensor, _, _ = self.mappo.select_action(
                    obs_tensor, deterministic=True
                )

                env_actions = {}
                for i in range(self.num_agents):
                    env_actions[f'agent_{i}'] = {
                        'lane': actions_tensor['lane'][i].cpu().item(),
                        'direction': actions_tensor['direction'][i].cpu().item(),
                        'motion': actions_tensor['motion'][i].cpu().numpy()
                    }

                obs_dict, reward_dict, terminated_dict, truncated_dict, info_dict = self.env.step(env_actions)

                reward_batch = np.array([
                    reward_dict[f'agent_{i}'] for i in range(self.num_agents)
                ])
                episode_reward += reward_batch.mean()

                done = terminated_dict['__all__'] or truncated_dict['__all__']

            eval_rewards.append(episode_reward)
            eval_tasks.append(info_dict.get('agent_0', {}).get('completed_tasks', 0))

        return np.mean(eval_rewards), np.mean(eval_tasks)

    def save_training_curves(self):
        """ä¿å­˜è®­ç»ƒæ›²çº¿"""
        import matplotlib.pyplot as plt

        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        # å¥–åŠ±æ›²çº¿
        ax = axes[0]
        ax.plot(self.episode_rewards, alpha=0.6, label='Episode Reward')
        window = 50
        if len(self.episode_rewards) >= window:
            moving_avg = np.convolve(
                self.episode_rewards,
                np.ones(window) / window,
                mode='valid'
            )
            ax.plot(
                range(window - 1, len(self.episode_rewards)),
                moving_avg,
                'r-',
                linewidth=2,
                label=f'Moving Average ({window})'
            )
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.set_title('Training Curve - Rewards (h_bi)')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # ä»»åŠ¡å®Œæˆæ•°æ›²çº¿
        ax = axes[1]
        ax.plot(self.episode_tasks_completed, alpha=0.6, color='green', label='Tasks Completed')
        if len(self.episode_tasks_completed) >= window:
            moving_avg_tasks = np.convolve(
                self.episode_tasks_completed,
                np.ones(window) / window,
                mode='valid'
            )
            ax.plot(
                range(window - 1, len(self.episode_tasks_completed)),
                moving_avg_tasks,
                'r-',
                linewidth=2,
                label=f'Moving Average ({window})'
            )
        ax.set_xlabel('Episode')
        ax.set_ylabel('Tasks Completed')
        ax.set_title('Training Curve - Task Completion (h_bi)')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()

        curve_path = os.path.join(train_config.LOG_DIR, 'training_curves.png')
        plt.savefig(curve_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curve_path}")
        plt.close()

        # ä¿å­˜æ•°æ®
        data = {
            'episode_rewards': self.episode_rewards,
            'episode_tasks_completed': self.episode_tasks_completed,
            'config': {
                'experiment_name': train_config.EXPERIMENT_NAME,
                'experiment_desc': train_config.EXPERIMENT_DESC,
                'bidirectional': env_config.BIDIRECTIONAL,
                'num_episodes': train_config.NUM_EPISODES
            }
        }

        json_path = os.path.join(train_config.LOG_DIR, 'training_data.json')
        with open(json_path, 'w') as f:
            json.dump(data, f, indent=2)
        print(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜: {json_path}")


def main():
    """ä¸»å‡½æ•°"""
    trainer = Trainer()
    trainer.train()


if __name__ == "__main__":
    main()