"""
å¯¹æ¯”åˆ†æè„šæœ¬ v2.2ï¼šæ°´å¹³åŒå‘ vs æ°´å¹³å•å‘
âœ¨ æ–°å¢ï¼šä»»åŠ¡å®Œæˆæ—¶é—´å¯¹æ¯”

è¿è¡Œæ–¹æ³•ï¼š
    python compare_results_v2.py

åŠŸèƒ½ï¼š
1. åŠ è½½ä¸¤ä¸ªå®éªŒçš„è¯„ä¼°ç»“æœï¼ˆåŒ…å«ä»»åŠ¡æ—¶é—´ï¼‰
2. ç”Ÿæˆå¯¹æ¯”å›¾è¡¨ï¼ˆåŒ…å«æ—¶é—´å¯¹æ¯”ï¼‰
3. è¾“å‡ºè¯¦ç»†çš„ç»Ÿè®¡å¯¹æ¯”ï¼ˆåŒ…å«æ—¶é—´æ•ˆç‡ï¼‰
4. ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path

sys.path.append(os.path.dirname(os.path.abspath(__file__)))


class ResultComparatorV2:
    """ç»“æœå¯¹æ¯”åˆ†æå™¨ v2.2 - åŒ…å«ä»»åŠ¡æ—¶é—´"""

    def __init__(self):
        """åˆå§‹åŒ–å¯¹æ¯”åˆ†æå™¨"""
        self.results = {}

    def load_results(self, exp_name: str, result_path: str):
        """åŠ è½½å®éªŒç»“æœ"""
        if not os.path.exists(result_path):
            print(f"âš ï¸  æ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶: {result_path}")
            return False

        with open(result_path, 'r') as f:
            self.results[exp_name] = json.load(f)

        print(f"âœ… å·²åŠ è½½: {exp_name} - {result_path}")
        return True

    def print_comparison_table(self):
        """æ‰“å°å¯¹æ¯”è¡¨æ ¼ - åŒ…å«ä»»åŠ¡æ—¶é—´"""
        if len(self.results) < 2:
            print("âŒ éœ€è¦è‡³å°‘ä¸¤ä¸ªå®éªŒç»“æœæ‰èƒ½å¯¹æ¯”")
            return

        print("\n" + "=" * 80)
        print("ğŸ“Š å®éªŒå¯¹æ¯”ç»Ÿè®¡è¡¨ v2.2")
        print("=" * 80)

        # è·å–æ‰€æœ‰æŒ‡æ ‡ï¼ˆåŒ…å«ä»»åŠ¡æ—¶é—´ï¼‰
        metrics = ['episode_rewards', 'tasks_completed', 'task_completion_times',  # âœ¨ æ–°å¢
                   'episode_lengths', 'collisions', 'direction_changes']

        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        data = []
        for metric in metrics:
            row = {'Metric': metric.replace('_', ' ').title()}

            for exp_name, result in self.results.items():
                if metric in result['statistics']:
                    stats = result['statistics'][metric]
                    row[f"{exp_name}_mean"] = f"{stats['mean']:.2f}"
                    row[f"{exp_name}_std"] = f"{stats['std']:.2f}"

            data.append(row)

        # æ‰“å°è¡¨æ ¼
        df = pd.DataFrame(data)
        print(df.to_string(index=False))
        print("=" * 80)

        # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
        if 'h_bi' in self.results and 'h_uni' in self.results:
            print("\nğŸ“ˆ åŒå‘ vs å•å‘ æ”¹è¿›åˆ†æ")
            print("=" * 80)

            # ä»»åŠ¡å®Œæˆæ•°
            bi_tasks = self.results['h_bi']['statistics']['tasks_completed']['mean']
            uni_tasks = self.results['h_uni']['statistics']['tasks_completed']['mean']
            task_improvement = ((bi_tasks - uni_tasks) / uni_tasks * 100) if uni_tasks > 0 else 0

            # å¹³å‡å¥–åŠ±
            bi_reward = self.results['h_bi']['statistics']['episode_rewards']['mean']
            uni_reward = self.results['h_uni']['statistics']['episode_rewards']['mean']
            reward_improvement = ((bi_reward - uni_reward) / abs(uni_reward) * 100) if uni_reward != 0 else 0

            # ç¢°æ’æ¬¡æ•°
            bi_collisions = self.results['h_bi']['statistics']['collisions']['mean']
            uni_collisions = self.results['h_uni']['statistics']['collisions']['mean']
            collision_reduction = ((uni_collisions - bi_collisions) / uni_collisions * 100) if uni_collisions > 0 else 0

            # âœ¨ ä»»åŠ¡å®Œæˆæ—¶é—´ï¼ˆæ–°å¢ï¼‰
            if 'task_completion_times' in self.results['h_bi']['statistics'] and \
                    'task_completion_times' in self.results['h_uni']['statistics']:
                bi_time = self.results['h_bi']['statistics']['task_completion_times']['mean']
                uni_time = self.results['h_uni']['statistics']['task_completion_times']['mean']
                time_improvement = ((uni_time - bi_time) / uni_time * 100) if uni_time > 0 else 0
            else:
                bi_time = uni_time = time_improvement = None

            print(f"ä»»åŠ¡å®Œæˆæ•°:")
            print(f"  åŒå‘: {bi_tasks:.2f}")
            print(f"  å•å‘: {uni_tasks:.2f}")
            print(f"  æ”¹è¿›: {task_improvement:+.1f}%")

            print(f"\nå¹³å‡å¥–åŠ±:")
            print(f"  åŒå‘: {bi_reward:.2f}")
            print(f"  å•å‘: {uni_reward:.2f}")
            print(f"  æ”¹è¿›: {reward_improvement:+.1f}%")

            print(f"\nç¢°æ’æ¬¡æ•°:")
            print(f"  åŒå‘: {bi_collisions:.2f}")
            print(f"  å•å‘: {uni_collisions:.2f}")
            print(f"  å‡å°‘: {collision_reduction:.1f}%")

            # âœ¨ ä»»åŠ¡å®Œæˆæ—¶é—´å¯¹æ¯”ï¼ˆæ–°å¢ï¼‰
            if bi_time is not None and uni_time is not None:
                print(f"\nâ±ï¸  ä»»åŠ¡å®Œæˆæ—¶é—´: âœ¨")
                print(f"  åŒå‘: {bi_time:.1f}ç§’")
                print(f"  å•å‘: {uni_time:.1f}ç§’")
                if time_improvement > 0:
                    print(f"  æ”¹è¿›: åŒå‘å¿« {time_improvement:.1f}% âš¡")
                elif time_improvement < 0:
                    print(f"  ç»“æœ: å•å‘å¿« {-time_improvement:.1f}%")
                else:
                    print(f"  ç»“æœ: ä¸¤è€…ç›¸å½“")

            # åŒå‘ç‰¹æœ‰æŒ‡æ ‡
            if 'backward_usage' in self.results['h_bi']['statistics']:
                backward = self.results['h_bi']['statistics']['backward_usage']['mean']
                print(f"\nâœ¨ åŒå‘è·¯ç”±ç‰¹å¾:")
                print(f"  åé€€ä½¿ç”¨ç‡: {backward:.1f}%")
                print(f"  è¯´æ˜: AGVåˆ©ç”¨åé€€åŠŸèƒ½æé«˜çµæ´»æ€§")

            print("=" * 80)

    def plot_comparison(self):
        """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨ - åŒ…å«ä»»åŠ¡æ—¶é—´å¯¹æ¯”"""
        if len(self.results) < 2:
            print("âŒ éœ€è¦è‡³å°‘ä¸¤ä¸ªå®éªŒç»“æœæ‰èƒ½å¯¹æ¯”")
            return

        # âœ¨ æ‰©å±•ä¸º3x3å¸ƒå±€ï¼ˆ9ä¸ªå›¾ï¼‰
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        fig.suptitle('Experimental Comparison v2.2: h_bi vs h_uni (with Task Time)',
                     fontsize=16, fontweight='bold')

        metrics = [
            ('episode_rewards', 'Episode Rewards', 0, 0),
            ('tasks_completed', 'Tasks Completed', 0, 1),
            ('episode_lengths', 'Episode Lengths', 0, 2),
            ('collisions', 'Collisions', 1, 0),
            ('direction_changes', 'Direction Changes', 1, 1)
        ]

        colors = {'h_bi': 'blue', 'h_uni': 'orange'}
        labels = {'h_bi': 'Bidirectional', 'h_uni': 'Unidirectional'}

        # ç»˜åˆ¶å‰5ä¸ªå¯¹æ¯”å›¾
        for metric, title, row, col in metrics:
            ax = axes[row, col]

            for exp_name, result in self.results.items():
                if metric in result['raw_data']:
                    data = result['raw_data'][metric]
                    ax.plot(data, alpha=0.6, label=labels.get(exp_name, exp_name),
                            color=colors.get(exp_name, 'gray'))
                    ax.axhline(np.mean(data), color=colors.get(exp_name, 'gray'),
                               linestyle='--', alpha=0.8, linewidth=1.5)

            ax.set_xlabel('Episode')
            ax.set_ylabel(title)
            ax.set_title(title)
            ax.legend()
            ax.grid(True, alpha=0.3)

        # 6. ä»»åŠ¡å®Œæˆç®±çº¿å›¾
        ax = axes[1, 2]
        if 'h_bi' in self.results and 'h_uni' in self.results:
            bi_tasks = self.results['h_bi']['raw_data']['tasks_completed']
            uni_tasks = self.results['h_uni']['raw_data']['tasks_completed']

            bp = ax.boxplot([bi_tasks, uni_tasks],
                            labels=['Bidirectional', 'Unidirectional'],
                            patch_artist=True)
            bp['boxes'][0].set_facecolor('lightblue')
            bp['boxes'][1].set_facecolor('lightyellow')

            ax.set_ylabel('Tasks Completed')
            ax.set_title('Task Completion Distribution')
            ax.grid(True, alpha=0.3)

        # âœ¨ 7. ä»»åŠ¡å®Œæˆæ—¶é—´åˆ†å¸ƒå¯¹æ¯”ï¼ˆæ–°å¢ï¼‰
        ax = axes[2, 0]
        if ('h_bi' in self.results and 'h_uni' in self.results and
                'task_completion_times' in self.results['h_bi']['raw_data'] and
                'task_completion_times' in self.results['h_uni']['raw_data']):

            bi_times = self.results['h_bi']['raw_data']['task_completion_times']
            uni_times = self.results['h_uni']['raw_data']['task_completion_times']

            if bi_times and uni_times:
                # ä½¿ç”¨ç›¸åŒçš„binsèŒƒå›´
                all_times = bi_times + uni_times
                bins = np.linspace(min(all_times), max(all_times), 30)

                ax.hist(bi_times, bins=bins, alpha=0.5, color='blue',
                        label='Bidirectional', edgecolor='black')
                ax.hist(uni_times, bins=bins, alpha=0.5, color='orange',
                        label='Unidirectional', edgecolor='black')

                ax.axvline(np.mean(bi_times), color='blue', linestyle='--', linewidth=2)
                ax.axvline(np.mean(uni_times), color='orange', linestyle='--', linewidth=2)

                ax.set_xlabel('Task Completion Time (seconds)')
                ax.set_ylabel('Frequency')
                ax.set_title('â±ï¸  Task Time Distribution âœ¨')
                ax.legend()
                ax.grid(True, alpha=0.3)

        # âœ¨ 8. ä»»åŠ¡å®Œæˆæ—¶é—´ç®±çº¿å›¾å¯¹æ¯”ï¼ˆæ–°å¢ï¼‰
        ax = axes[2, 1]
        if ('h_bi' in self.results and 'h_uni' in self.results and
                'task_completion_times' in self.results['h_bi']['raw_data'] and
                'task_completion_times' in self.results['h_uni']['raw_data']):

            bi_times = self.results['h_bi']['raw_data']['task_completion_times']
            uni_times = self.results['h_uni']['raw_data']['task_completion_times']

            if bi_times and uni_times:
                bp = ax.boxplot([bi_times, uni_times],
                                labels=['Bidirectional', 'Unidirectional'],
                                patch_artist=True)
                bp['boxes'][0].set_facecolor('lightblue')
                bp['boxes'][1].set_facecolor('lightyellow')

                # æ·»åŠ å‡å€¼æ ‡æ³¨
                means = [np.mean(bi_times), np.mean(uni_times)]
                ax.plot([1, 2], means, 'ro-', linewidth=2,
                        markersize=8, label='Mean')

                ax.set_ylabel('Task Completion Time (seconds)')
                ax.set_title('â±ï¸  Task Time Comparison âœ¨')
                ax.legend()
                ax.grid(True, alpha=0.3, axis='y')

                # æ·»åŠ æ”¹è¿›ç™¾åˆ†æ¯”æ–‡æœ¬
                improvement = ((means[1] - means[0]) / means[1] * 100)
                text = f"Bi faster: {improvement:.1f}%" if improvement > 0 else f"Uni faster: {-improvement:.1f}%"
                ax.text(0.5, 0.95, text, transform=ax.transAxes,
                        fontsize=10, ha='center', va='top',
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        # âœ¨ 9. ä»»åŠ¡æ•ˆç‡ç»¼åˆå¯¹æ¯”ï¼ˆæ–°å¢ï¼‰
        ax = axes[2, 2]
        if 'h_bi' in self.results and 'h_uni' in self.results:
            metrics_names = ['Tasks/Episode', 'Avg Time (s)', 'Collisions']

            bi_values = [
                self.results['h_bi']['statistics']['tasks_completed']['mean'],
                self.results['h_bi']['statistics'].get('task_completion_times', {}).get('mean', 0),
                self.results['h_bi']['statistics']['collisions']['mean']
            ]

            uni_values = [
                self.results['h_uni']['statistics']['tasks_completed']['mean'],
                self.results['h_uni']['statistics'].get('task_completion_times', {}).get('mean', 0),
                self.results['h_uni']['statistics']['collisions']['mean']
            ]

            x = np.arange(len(metrics_names))
            width = 0.35

            # å½’ä¸€åŒ–æ˜¾ç¤ºï¼ˆä»¥å•å‘ä¸ºåŸºå‡†ï¼‰
            normalized_bi = []
            normalized_uni = []
            for bi, uni in zip(bi_values, uni_values):
                if uni > 0:
                    normalized_bi.append(bi / uni * 100)
                    normalized_uni.append(100)
                else:
                    normalized_bi.append(0)
                    normalized_uni.append(0)

            ax.bar(x - width / 2, normalized_bi, width, label='Bidirectional',
                   color='lightblue', edgecolor='black')
            ax.bar(x + width / 2, normalized_uni, width, label='Unidirectional',
                   color='lightyellow', edgecolor='black')

            ax.axhline(100, color='gray', linestyle='--', alpha=0.5)
            ax.set_ylabel('Normalized Performance (%)')
            ax.set_title('âš¡ Efficiency Comparison (Uni=100%)')
            ax.set_xticks(x)
            ax.set_xticklabels(metrics_names, rotation=15, ha='right')
            ax.legend()
            ax.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        save_path = './data/comparison_results_v2.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {save_path}")

        plt.show()

    def generate_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š - åŒ…å«ä»»åŠ¡æ—¶é—´"""
        if len(self.results) < 2:
            print("âŒ éœ€è¦è‡³å°‘ä¸¤ä¸ªå®éªŒç»“æœæ‰èƒ½ç”ŸæˆæŠ¥å‘Š")
            return

        report = []
        report.append("# å®éªŒå¯¹æ¯”æŠ¥å‘Š v2.2ï¼šæ°´å¹³åŒå‘ vs æ°´å¹³å•å‘")
        report.append("\n## å®éªŒé…ç½®")

        for exp_name, result in self.results.items():
            report.append(f"\n### {exp_name}")
            report.append(f"- æè¿°: {result.get('description', 'N/A')}")
            if 'config' in result:
                config = result['config']
                report.append(f"- åŒå‘è·¯ç”±: {'å¯ç”¨' if config.get('bidirectional') else 'ç¦ç”¨'}")
                report.append(f"- AGVæ•°é‡: {config.get('num_agents', 'N/A')}")
                report.append(f"- è½¦é“æ•°é‡: {config.get('num_lanes', 'N/A')}")

        report.append("\n## æ€§èƒ½å¯¹æ¯”")
        report.append("\n| æŒ‡æ ‡ | åŒå‘ (h_bi) | å•å‘ (h_uni) | æ”¹è¿› |")
        report.append("|------|-------------|--------------|------|")

        if 'h_bi' in self.results and 'h_uni' in self.results:
            metrics = [
                ('episode_rewards', 'å¹³å‡å¥–åŠ±'),
                ('tasks_completed', 'ä»»åŠ¡å®Œæˆæ•°'),
                ('task_completion_times', 'â±ï¸  ä»»åŠ¡å®Œæˆæ—¶é—´(ç§’)'),  # âœ¨ æ–°å¢
                ('episode_lengths', 'Episodeé•¿åº¦'),
                ('collisions', 'ç¢°æ’æ¬¡æ•°')
            ]

            for metric_key, metric_name in metrics:
                if (metric_key in self.results['h_bi']['statistics'] and
                        metric_key in self.results['h_uni']['statistics']):

                    bi_val = self.results['h_bi']['statistics'][metric_key]['mean']
                    uni_val = self.results['h_uni']['statistics'][metric_key]['mean']

                    if metric_key in ['collisions', 'episode_lengths', 'task_completion_times']:
                        # è¿™äº›æŒ‡æ ‡ï¼šè¶Šå°‘è¶Šå¥½
                        improvement = ((uni_val - bi_val) / uni_val * 100) if uni_val > 0 else 0
                        if improvement > 0:
                            improvement_str = f"{improvement:.1f}% å‡å°‘ âœ…"
                        else:
                            improvement_str = f"{-improvement:.1f}% å¢åŠ "
                    else:
                        # å…¶ä»–æŒ‡æ ‡ï¼šè¶Šå¤šè¶Šå¥½
                        improvement = ((bi_val - uni_val) / abs(uni_val) * 100) if uni_val != 0 else 0
                        improvement_str = f"{improvement:+.1f}%"

                    report.append(f"| {metric_name} | {bi_val:.2f} | {uni_val:.2f} | {improvement_str} |")

        report.append("\n## ç»“è®º")
        report.append("\nåŸºäºå®éªŒç»“æœï¼ŒåŒå‘è·¯ç”±ç›¸æ¯”å•å‘è·¯ç”±çš„ä¼˜åŠ¿ï¼š")

        if 'h_bi' in self.results and 'h_uni' in self.results:
            bi_tasks = self.results['h_bi']['statistics']['tasks_completed']['mean']
            uni_tasks = self.results['h_uni']['statistics']['tasks_completed']['mean']

            if bi_tasks > uni_tasks:
                report.append("- âœ… ä»»åŠ¡å®Œæˆæ•°æ›´é«˜ï¼Œè¡¨æ˜åŒå‘è·¯ç”±æé«˜äº†è°ƒåº¦æ•ˆç‡")

            if 'task_completion_times' in self.results['h_bi']['statistics']:
                bi_time = self.results['h_bi']['statistics']['task_completion_times']['mean']
                uni_time = self.results['h_uni']['statistics']['task_completion_times']['mean']

                if bi_time < uni_time:
                    time_improv = (uni_time - bi_time) / uni_time * 100
                    report.append(f"- âœ… â±ï¸  ä»»åŠ¡å®Œæˆæ—¶é—´æ›´çŸ­ï¼ˆå¿«{time_improv:.1f}%ï¼‰ï¼Œè°ƒåº¦æ•ˆç‡æ›´é«˜")

            if 'backward_usage' in self.results['h_bi']['statistics']:
                backward = self.results['h_bi']['statistics']['backward_usage']['mean']
                report.append(f"- âœ… AGVåˆ©ç”¨åé€€åŠŸèƒ½ï¼ˆä½¿ç”¨ç‡{backward:.1f}%ï¼‰ï¼Œæé«˜çµæ´»æ€§")

            bi_collisions = self.results['h_bi']['statistics']['collisions']['mean']
            uni_collisions = self.results['h_uni']['statistics']['collisions']['mean']

            if bi_collisions < uni_collisions:
                report.append("- âœ… ç¢°æ’æ¬¡æ•°æ›´å°‘ï¼Œåé€€åŠŸèƒ½æœ‰åŠ©äºé¿éšœ")

        report_text = "\n".join(report)

        # ä¿å­˜æŠ¥å‘Š
        report_path = './data/comparison_report_v2.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)

        print(f"\nâœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # æ‰“å°æŠ¥å‘Š
        print("\n" + "=" * 80)
        print(report_text)
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("ğŸ“Š å®éªŒç»“æœå¯¹æ¯”åˆ†æå·¥å…· v2.2 (with Task Time)")
    print("=" * 80)

    comparator = ResultComparatorV2()

    # åŠ è½½å®éªŒç»“æœï¼ˆä¼˜å…ˆåŠ è½½v2ç‰ˆæœ¬ï¼‰
    results_loaded = 0

    # å°è¯•åŠ è½½æ°´å¹³åŒå‘ç»“æœï¼ˆv2ä¼˜å…ˆï¼‰
    h_bi_paths = [
        './data/logs_h_bi/evaluation_results_h_bi_v2.json',
        './data/logs_h_bi/evaluation_results_h_bi.json'
    ]
    for path in h_bi_paths:
        if comparator.load_results('h_bi', path):
            results_loaded += 1
            break

    # å°è¯•åŠ è½½æ°´å¹³å•å‘ç»“æœï¼ˆv2ä¼˜å…ˆï¼‰
    h_uni_paths = [
        './data/logs_h_uni/evaluation_results_h_uni_v2.json',
        './data/logs_h_uni/evaluation_results_h_uni.json'
    ]
    for path in h_uni_paths:
        if comparator.load_results('h_uni', path):
            results_loaded += 1
            break

    if results_loaded < 2:
        print("\nâŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¶³å¤Ÿçš„è¯„ä¼°ç»“æœæ–‡ä»¶")
        print("\nè¯·å…ˆè¿è¡Œè¯„ä¼°è„šæœ¬ï¼š")
        print("  python evaluate_h_bi_v2.py --checkpoint xxx.pt --episodes 50 --exp h_bi")
        print("  python evaluate_h_uni_v2.py --checkpoint xxx.pt --episodes 50 --exp h_uni")
        return

    print(f"\nâœ… æˆåŠŸåŠ è½½ {results_loaded} ä¸ªå®éªŒç»“æœ")

    # ç”Ÿæˆå¯¹æ¯”åˆ†æ
    print("\n1ï¸âƒ£  ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼...")
    comparator.print_comparison_table()

    print("\n2ï¸âƒ£  ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    comparator.plot_comparison()

    print("\n3ï¸âƒ£  ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
    comparator.generate_report()

    print("\n" + "=" * 80)
    print("âœ… å¯¹æ¯”åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print("\nç”Ÿæˆçš„æ–‡ä»¶ï¼š")
    print("  - ./data/comparison_results_v2.png  (å¯¹æ¯”å›¾è¡¨)")
    print("  - ./data/comparison_report_v2.md    (å¯¹æ¯”æŠ¥å‘Š)")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()