"""
å®éªŒè¯„ä¼°è„šæœ¬ v2.2 - æ·»åŠ ä»»åŠ¡å®Œæˆæ—¶é—´è¿½è¸ª
é€‚ç”¨äº h_bi å’Œ h_uni

âœ¨ æ–°å¢åŠŸèƒ½ï¼š
1. æ”¶é›†ä»»åŠ¡å®Œæˆæ—¶é—´
2. ç»Ÿè®¡å¹³å‡å®Œæˆæ—¶é—´ã€æœ€çŸ­/æœ€é•¿æ—¶é—´
3. å¯è§†åŒ–ä»»åŠ¡æ—¶é—´åˆ†å¸ƒ
4. å¯¹æ¯”ä»»åŠ¡å®Œæˆæ•ˆç‡

ä½¿ç”¨æ–¹æ³•ï¼š
    # åŒå‘è¯„ä¼°
    python evaluate_h_bi_v2.py --checkpoint xxx.pt --episodes 50

    # å•å‘è¯„ä¼°ï¼ˆå¤åˆ¶å¹¶æ”¹åä¸º evaluate_h_uni_v2.pyï¼‰
    python evaluate_h_uni_v2.py --checkpoint xxx.pt --episodes 50
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import json
import argparse

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.env_config import env_config
from config.train_config_h_bi import train_config  # âœ¨ æ”¹ä¸º h_uni æ—¶ä¿®æ”¹è¿™é‡Œ
from config.model_config import model_config
from environment.port_env import PortEnvironment
from models.actor_critic import ActorCritic
from algorithm.mappo import MAPPO


class EvaluatorV2:
    """è¯„ä¼°å™¨ v2.2 - å¸¦ä»»åŠ¡æ—¶é—´è¿½è¸ª"""

    def __init__(self, checkpoint_path: str, experiment_name: str = "h_bi"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.experiment_name = experiment_name

        # è®¾ç½®åŒå‘/å•å‘æ¨¡å¼
        env_config.BIDIRECTIONAL = (experiment_name == "h_bi")
        env_config.VERBOSE = False

        self.device = torch.device(
            'cuda' if torch.cuda.is_available() and train_config.USE_CUDA
            else 'cpu'
        )

        mode_name = "åŒå‘è·¯ç”±" if env_config.BIDIRECTIONAL else "å•å‘è·¯ç”±"
        print(f"Using device: {self.device}")
        print(f"âœ… Evaluation Mode: æ°´å¹³å¸ƒå±€ + {mode_name} ({experiment_name})")
        print(f"âœ… BIDIRECTIONAL = {env_config.BIDIRECTIONAL}")

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = PortEnvironment(env_config)
        self.num_agents = env_config.NUM_AGVS

        # è®¡ç®—è§‚å¯Ÿç»´åº¦
        self.obs_dim = 7 + 5 * 4 + 6 + env_config.NUM_HORIZONTAL_LANES

        # åˆå§‹åŒ–æ¨¡å‹
        self.actor_critic = ActorCritic(
            obs_dim=self.obs_dim,
            actor_hidden_dims=model_config.ACTOR_HIDDEN_DIMS,
            critic_hidden_dims=model_config.CRITIC_HIDDEN_DIMS,
            num_lanes=env_config.NUM_HORIZONTAL_LANES,
            num_directions=2,
            use_centralized_critic=model_config.USE_CENTRALIZED_CRITIC
        )

        # åˆå§‹åŒ–MAPPO
        self.mappo = MAPPO(
            actor_critic=self.actor_critic,
            num_agents=self.num_agents,
            device=self.device
        )

        # åŠ è½½æ¨¡å‹
        if os.path.exists(checkpoint_path):
            self.mappo.load(checkpoint_path)
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {checkpoint_path}")
        else:
            print(f"âŒ æ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹: {checkpoint_path}")
            sys.exit(1)

        # âœ¨ è¯„ä¼°ç»“æœï¼ˆæ–°å¢ä»»åŠ¡æ—¶é—´ï¼‰
        self.eval_results = {
            'episode_rewards': [],
            'episode_lengths': [],
            'tasks_completed': [],
            'collisions': [],
            'direction_changes': [],
            'backward_usage': [] if env_config.BIDIRECTIONAL else None,
            'task_completion_times': []  # âœ¨ æ–°å¢ï¼šæ‰€æœ‰ä»»åŠ¡çš„å®Œæˆæ—¶é—´
        }

    def flatten_observation(self, obs_dict: dict) -> np.ndarray:
        """å±•å¹³è§‚å¯Ÿ"""
        obs_list = []
        obs_list.append(obs_dict['own_state'])
        obs_list.append(obs_dict['nearby_agvs'].flatten())
        obs_list.append(obs_dict['task_info'])
        obs_list.append(obs_dict['path_occupancy'])
        return np.concatenate(obs_list, axis=0)

    def evaluate_episode(self, render: bool = False) -> dict:
        """è¯„ä¼°å•ä¸ªepisode"""
        obs_dict, _ = self.env.reset()
        episode_reward = 0
        episode_length = 0
        done = False

        # è®°å½•æ–¹å‘å˜åŒ–å’Œåé€€ä½¿ç”¨
        direction_changes = [0] * self.num_agents
        backward_steps = [0] * self.num_agents if env_config.BIDIRECTIONAL else None
        total_steps = 0
        prev_directions = [agv.moving_forward for agv in self.env.agvs]

        # âœ¨ æ–°å¢ï¼šè®°å½•æœ¬episodeçš„ä»»åŠ¡å®Œæˆæ—¶é—´
        episode_task_times = []
        recorded_task_ids = set()  # é˜²æ­¢é‡å¤è®°å½•

        while not done:
            # æ”¶é›†è§‚å¯Ÿ
            obs_batch = []
            for i in range(self.num_agents):
                obs = self.flatten_observation(obs_dict[f'agent_{i}'])
                obs_batch.append(obs)

            obs_tensor = torch.FloatTensor(np.array(obs_batch)).to(self.device)

            # é€‰æ‹©åŠ¨ä½œ(ç¡®å®šæ€§)
            actions_tensor, _, _ = self.mappo.select_action(
                obs_tensor, deterministic=True
            )

            # è½¬æ¢ä¸ºç¯å¢ƒæ ¼å¼
            env_actions = {}
            for i in range(self.num_agents):
                env_actions[f'agent_{i}'] = {
                    'lane': actions_tensor['lane'][i].cpu().item(),
                    'direction': actions_tensor['direction'][i].cpu().item(),
                    'motion': actions_tensor['motion'][i].cpu().numpy()
                }

            # æ­¥è¿›
            obs_dict, reward_dict, terminated_dict, truncated_dict, info_dict = self.env.step(env_actions)

            # âœ¨ æ–°å¢ï¼šæ”¶é›†å·²å®Œæˆä»»åŠ¡çš„æ—¶é—´
            for task in self.env.completed_tasks:
                if (hasattr(task, 'completion_time') and
                        task.completion_time is not None and
                        task.id not in recorded_task_ids):
                    episode_task_times.append(task.completion_time)
                    recorded_task_ids.add(task.id)

            # ç»Ÿè®¡æ–¹å‘å˜åŒ–å’Œåé€€ä½¿ç”¨
            for i, agv in enumerate(self.env.agvs):
                if agv.moving_forward != prev_directions[i]:
                    direction_changes[i] += 1
                if env_config.BIDIRECTIONAL and not agv.moving_forward:
                    backward_steps[i] += 1
                prev_directions[i] = agv.moving_forward

            reward_batch = np.array([
                reward_dict[f'agent_{i}'] for i in range(self.num_agents)
            ])
            episode_reward += reward_batch.mean()
            episode_length += 1
            total_steps += 1

            done = terminated_dict['__all__'] or truncated_dict['__all__']

            if render:
                self.env.render()

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        info = info_dict.get('agent_0', {})

        result = {
            'reward': episode_reward,
            'length': episode_length,
            'tasks_completed': info.get('completed_tasks', 0),
            'collisions': info.get('collisions', 0),
            'direction_changes': np.mean(direction_changes),
            'task_times': episode_task_times  # âœ¨ æ–°å¢
        }

        if env_config.BIDIRECTIONAL:
            backward_usage = np.mean([b / max(total_steps, 1) for b in backward_steps]) * 100
            result['backward_usage'] = backward_usage

        return result

    def evaluate(self, num_episodes: int = 100, render: bool = False):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        mode_name = "åŒå‘" if env_config.BIDIRECTIONAL else "å•å‘"
        print("\n" + "=" * 60)
        print(f"ğŸ¯ å¼€å§‹è¯„ä¼°ï¼šæ°´å¹³{mode_name} ({self.experiment_name})")
        print("=" * 60)
        print(f"è¯„ä¼°è½®æ•°: {num_episodes}")
        print(f"AGVæ•°é‡: {self.num_agents}")
        print(f"åŒå‘è·¯ç”±: {'å¯ç”¨' if env_config.BIDIRECTIONAL else 'ç¦ç”¨'}")
        print("=" * 60 + "\n")

        for episode in tqdm(range(num_episodes), desc="Evaluating"):
            result = self.evaluate_episode(render=render)

            self.eval_results['episode_rewards'].append(result['reward'])
            self.eval_results['episode_lengths'].append(result['length'])
            self.eval_results['tasks_completed'].append(result['tasks_completed'])
            self.eval_results['collisions'].append(result['collisions'])
            self.eval_results['direction_changes'].append(result['direction_changes'])

            # âœ¨ æ”¶é›†æ‰€æœ‰ä»»åŠ¡å®Œæˆæ—¶é—´
            if result['task_times']:
                self.eval_results['task_completion_times'].extend(result['task_times'])

            if env_config.BIDIRECTIONAL:
                self.eval_results['backward_usage'].append(result['backward_usage'])

        # æ‰“å°ç»Ÿè®¡
        self.print_statistics()

        # å¯è§†åŒ–ç»“æœ
        self.visualize_results()

        # ä¿å­˜ç»“æœ
        self.save_results()

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        mode_name = "åŒå‘" if env_config.BIDIRECTIONAL else "å•å‘"
        print("\n" + "=" * 60)
        print(f"ğŸ“Š è¯„ä¼°ç»“æœç»Ÿè®¡ - æ°´å¹³{mode_name} ({self.experiment_name})")
        print("=" * 60)

        for key, values in self.eval_results.items():
            if values is None or len(values) == 0:
                continue

            mean_val = np.mean(values)
            std_val = np.std(values)
            min_val = np.min(values)
            max_val = np.max(values)

            display_name = key.replace('_', ' ').title()
            print(f"\n{display_name}:")
            print(f"  Mean: {mean_val:.2f}")
            print(f"  Std:  {std_val:.2f}")
            print(f"  Min:  {min_val:.2f}")
            print(f"  Max:  {max_val:.2f}")

        # âœ¨ ç‰¹åˆ«å¼ºè°ƒä»»åŠ¡å®Œæˆæ—¶é—´
        task_times = self.eval_results['task_completion_times']
        if task_times:
            print(f"\n{'=' * 60}")
            print(f"â±ï¸  ä»»åŠ¡å®Œæˆæ—¶é—´åˆ†æ âœ¨")
            print(f"{'=' * 60}")
            print(f"  æ€»å®Œæˆä»»åŠ¡æ•°: {len(task_times)}")
            print(f"  å¹³å‡å®Œæˆæ—¶é—´: {np.mean(task_times):.1f}ç§’")
            print(f"  æ ‡å‡†å·®: {np.std(task_times):.1f}ç§’")
            print(f"  æœ€çŸ­æ—¶é—´: {np.min(task_times):.1f}ç§’")
            print(f"  æœ€é•¿æ—¶é—´: {np.max(task_times):.1f}ç§’")
            print(f"  ä¸­ä½æ•°: {np.median(task_times):.1f}ç§’")

            # æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
            fast_tasks = sum(1 for t in task_times if t < 100)
            medium_tasks = sum(1 for t in task_times if 100 <= t < 200)
            slow_tasks = sum(1 for t in task_times if t >= 200)

            print(f"\n  æ—¶é—´åˆ†å¸ƒ:")
            print(f"    å¿«é€Ÿ(<100s): {fast_tasks} ({fast_tasks / len(task_times) * 100:.1f}%)")
            print(f"    ä¸­ç­‰(100-200s): {medium_tasks} ({medium_tasks / len(task_times) * 100:.1f}%)")
            print(f"    æ…¢é€Ÿ(>=200s): {slow_tasks} ({slow_tasks / len(task_times) * 100:.1f}%)")
            print(f"{'=' * 60}\n")

    def visualize_results(self):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœ - åŒ…å«ä»»åŠ¡æ—¶é—´å›¾è¡¨"""
        mode_name = "åŒå‘" if env_config.BIDIRECTIONAL else "å•å‘"

        # âœ¨ è°ƒæ•´ä¸º2x4å¸ƒå±€ï¼ˆ8ä¸ªå›¾ï¼‰
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        fig.suptitle(f'Evaluation Results - æ°´å¹³{mode_name} ({self.experiment_name}) v2.2',
                     fontsize=16, fontweight='bold')

        # å‰6ä¸ªå›¾ä¿æŒä¸å˜...
        # 1. Episode Rewards
        ax = axes[0, 0]
        ax.plot(self.eval_results['episode_rewards'], alpha=0.6)
        ax.axhline(np.mean(self.eval_results['episode_rewards']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.set_title('Episode Rewards')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. Tasks Completed
        ax = axes[0, 1]
        ax.plot(self.eval_results['tasks_completed'], alpha=0.6,
                color='green', marker='o')
        ax.axhline(np.mean(self.eval_results['tasks_completed']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Tasks')
        ax.set_title('Tasks Completed')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 3. Episode Lengths
        ax = axes[0, 2]
        ax.plot(self.eval_results['episode_lengths'], alpha=0.6, color='orange')
        ax.axhline(np.mean(self.eval_results['episode_lengths']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Length (steps)')
        ax.set_title('Episode Lengths')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 4. Collisions
        ax = axes[0, 3]
        ax.plot(self.eval_results['collisions'], alpha=0.6, color='red')
        ax.axhline(np.mean(self.eval_results['collisions']),
                   color='darkred', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Collisions')
        ax.set_title('Collision Count')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 5. Direction Changes or Backward Usage
        ax = axes[1, 0]
        if env_config.BIDIRECTIONAL and self.eval_results['backward_usage']:
            ax.plot(self.eval_results['backward_usage'], alpha=0.6,
                    color='purple', marker='s')
            ax.axhline(np.mean(self.eval_results['backward_usage']),
                       color='r', linestyle='--', label='Mean')
            ax.set_ylabel('Backward Usage (%)')
            ax.set_title('Backward Usage Rate')
        else:
            ax.plot(self.eval_results['direction_changes'], alpha=0.6, color='purple')
            ax.axhline(np.mean(self.eval_results['direction_changes']),
                       color='r', linestyle='--', label='Mean')
            ax.set_ylabel('Direction Changes')
            ax.set_title('Direction Changes')
        ax.set_xlabel('Episode')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 6. Task Distribution
        ax = axes[1, 1]
        ax.hist(self.eval_results['tasks_completed'], bins=20,
                alpha=0.7, color='skyblue', edgecolor='black')
        ax.axvline(np.mean(self.eval_results['tasks_completed']),
                   color='r', linestyle='--', linewidth=2, label='Mean')
        ax.set_xlabel('Tasks Completed')
        ax.set_ylabel('Frequency')
        ax.set_title('Task Completion Distribution')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # âœ¨ 7. ä»»åŠ¡å®Œæˆæ—¶é—´åˆ†å¸ƒï¼ˆæ–°å¢ï¼‰
        ax = axes[1, 2]
        task_times = self.eval_results['task_completion_times']
        if task_times:
            ax.hist(task_times, bins=30, alpha=0.7, color='green', edgecolor='black')
            ax.axvline(np.mean(task_times), color='r', linestyle='--',
                       linewidth=2, label=f'Mean: {np.mean(task_times):.1f}s')
            ax.axvline(np.median(task_times), color='orange', linestyle=':',
                       linewidth=2, label=f'Median: {np.median(task_times):.1f}s')
            ax.set_xlabel('Task Completion Time (seconds)')
            ax.set_ylabel('Frequency')
            ax.set_title('â±ï¸  Task Time Distribution âœ¨')
            ax.legend()
            ax.grid(True, alpha=0.3)
        else:
            ax.text(0.5, 0.5, 'No Task Time Data', ha='center', va='center',
                    transform=ax.transAxes, fontsize=12)

        # âœ¨ 8. ä»»åŠ¡å®Œæˆæ—¶é—´ç®±çº¿å›¾ï¼ˆæ–°å¢ï¼‰
        ax = axes[1, 3]
        if task_times:
            bp = ax.boxplot([task_times], labels=['Task Time'],
                            patch_artist=True, widths=0.5)
            bp['boxes'][0].set_facecolor('lightgreen')
            ax.set_ylabel('Task Completion Time (seconds)')
            ax.set_title('â±ï¸  Task Time Statistics âœ¨')
            ax.grid(True, alpha=0.3, axis='y')

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
            stats_text = f"Mean: {np.mean(task_times):.1f}s\n"
            stats_text += f"Median: {np.median(task_times):.1f}s\n"
            stats_text += f"Min: {np.min(task_times):.1f}s\n"
            stats_text += f"Max: {np.max(task_times):.1f}s"
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
                    fontsize=9, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        else:
            ax.text(0.5, 0.5, 'No Task Time Data', ha='center', va='center',
                    transform=ax.transAxes, fontsize=12)

        plt.tight_layout()

        save_path = os.path.join(train_config.LOG_DIR, f'evaluation_results_{self.experiment_name}_v2.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")

        plt.show()

    def save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        mode_name = "åŒå‘" if env_config.BIDIRECTIONAL else "å•å‘"

        # å¤„ç†å¯èƒ½ä¸ºNoneçš„å€¼
        processed_results = {}
        for key, values in self.eval_results.items():
            if values is None or len(values) == 0:
                continue
            processed_results[key] = {
                'mean': float(np.mean(values)),
                'std': float(np.std(values)),
                'min': float(np.min(values)),
                'max': float(np.max(values))
            }

        results = {
            'experiment': self.experiment_name,
            'description': f'æ°´å¹³å¸ƒå±€ + {mode_name}',
            'statistics': processed_results,
            'raw_data': {
                key: [float(v) for v in values]
                for key, values in self.eval_results.items()
                if values is not None and len(values) > 0
            },
            'config': {
                'num_agents': self.num_agents,
                'bidirectional': env_config.BIDIRECTIONAL,
                'num_lanes': env_config.NUM_HORIZONTAL_LANES,
            }
        }

        save_path = os.path.join(train_config.LOG_DIR, f'evaluation_results_{self.experiment_name}_v2.json')
        with open(save_path, 'w') as f:
            json.dump(results, f, indent=2)

        print(f"âœ… è¯„ä¼°æ•°æ®å·²ä¿å­˜: {save_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Evaluate Model with Task Time Tracking (v2.2)')
    parser.add_argument(
        '--checkpoint',
        type=str,
        required=True,
        help='Path to model checkpoint'
    )
    parser.add_argument(
        '--episodes',
        type=int,
        default=50,
        help='Number of evaluation episodes'
    )
    parser.add_argument(
        '--render',
        action='store_true',
        help='Render environment'
    )
    parser.add_argument(
        '--exp',
        type=str,
        default='h_bi',
        choices=['h_bi', 'h_uni'],
        help='Experiment name (h_bi or h_uni)'
    )

    args = parser.parse_args()

    # è¿è¡Œè¯„ä¼°
    evaluator = EvaluatorV2(args.checkpoint, experiment_name=args.exp)
    evaluator.evaluate(num_episodes=args.episodes, render=args.render)


if __name__ == "__main__":
    main()