"""
å®éªŒ2è¯„ä¼°è„šæœ¬ï¼šæ°´å¹³å¸ƒå±€ + å•å‘è·¯ç”±
Horizontal Layout + Unidirectional Routing - Evaluation

è¿è¡Œæ–¹æ³•ï¼š
    python evaluate_h_uni.py --checkpoint ./data/checkpoints_h_uni/mappo_final_1000ep.pt --episodes 100

âœ¨ å…³é”®å·®å¼‚ï¼š
- ä½¿ç”¨ train_config_h_uni é…ç½®
- è®¾ç½® BIDIRECTIONAL = False
- ä¸è®°å½•åé€€ä½¿ç”¨ç‡ï¼ˆå•å‘æ¨¡å¼ç‰¹å¾ï¼‰
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import json
import argparse

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.env_config import env_config
from config.train_config_h_uni import train_config  # âœ¨ ä½¿ç”¨æ°´å¹³å•å‘é…ç½®
from config.model_config import model_config
from environment.port_env import PortEnvironment
from models.actor_critic import ActorCritic
from algorithm.mappo import MAPPO


class Evaluator:
    """è¯„ä¼°å™¨ç±» - æ°´å¹³å•å‘"""

    def __init__(self, checkpoint_path: str):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        # âœ¨ ç¡®ä¿ä½¿ç”¨å•å‘æ¨¡å¼
        env_config.BIDIRECTIONAL = False  # âœ¨âœ¨ å…³é”®å·®å¼‚
        env_config.VERBOSE = False

        self.device = torch.device(
            'cuda' if torch.cuda.is_available() and train_config.USE_CUDA
            else 'cpu'
        )
        print(f"Using device: {self.device}")
        print(f"âœ… Evaluation Mode: æ°´å¹³å¸ƒå±€ + å•å‘è·¯ç”± (h_uni)")
        print(f"âš ï¸  BIDIRECTIONAL = {env_config.BIDIRECTIONAL}")
        print(f"âš ï¸  AGVåªèƒ½å‰è¿›ï¼Œä¸èƒ½åé€€")

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = PortEnvironment(env_config)
        self.num_agents = env_config.NUM_AGVS

        # è®¡ç®—è§‚å¯Ÿç»´åº¦
        self.obs_dim = 7 + 5 * 4 + 6 + env_config.NUM_HORIZONTAL_LANES

        # åˆå§‹åŒ–æ¨¡å‹
        self.actor_critic = ActorCritic(
            obs_dim=self.obs_dim,
            actor_hidden_dims=model_config.ACTOR_HIDDEN_DIMS,
            critic_hidden_dims=model_config.CRITIC_HIDDEN_DIMS,
            num_lanes=env_config.NUM_HORIZONTAL_LANES,
            num_directions=2,
            use_centralized_critic=model_config.USE_CENTRALIZED_CRITIC
        )

        # åˆå§‹åŒ–MAPPO
        self.mappo = MAPPO(
            actor_critic=self.actor_critic,
            num_agents=self.num_agents,
            device=self.device
        )

        # åŠ è½½æ¨¡å‹
        if os.path.exists(checkpoint_path):
            self.mappo.load(checkpoint_path)
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {checkpoint_path}")
        else:
            print(f"âŒ æ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹: {checkpoint_path}")
            sys.exit(1)

        # è¯„ä¼°ç»“æœï¼ˆä¸è®°å½•åé€€ä½¿ç”¨ç‡ï¼‰
        self.eval_results = {
            'episode_rewards': [],
            'episode_lengths': [],
            'tasks_completed': [],
            'collisions': [],
            'direction_changes': [],
            # âœ¨ å•å‘æ¨¡å¼ä¸è®°å½•åé€€ä½¿ç”¨ç‡
            'task_completion_times': []  # âœ¨ æ–°å¢ï¼šä»»åŠ¡å®Œæˆæ—¶é—´åˆ—è¡¨
        }

    def flatten_observation(self, obs_dict: dict) -> np.ndarray:
        """å±•å¹³è§‚å¯Ÿ"""
        obs_list = []
        obs_list.append(obs_dict['own_state'])
        obs_list.append(obs_dict['nearby_agvs'].flatten())
        obs_list.append(obs_dict['task_info'])
        obs_list.append(obs_dict['path_occupancy'])
        return np.concatenate(obs_list, axis=0)

    def evaluate_episode(self, render: bool = False) -> dict:
        """è¯„ä¼°å•ä¸ªepisode"""
        obs_dict, _ = self.env.reset()
        episode_reward = 0
        episode_length = 0
        done = False

        # è®°å½•æ–¹å‘å˜åŒ–
        direction_changes = [0] * self.num_agents
        prev_directions = [agv.moving_forward for agv in self.env.agvs]

        while not done:
            # æ”¶é›†è§‚å¯Ÿ
            obs_batch = []
            for i in range(self.num_agents):
                obs = self.flatten_observation(obs_dict[f'agent_{i}'])
                obs_batch.append(obs)

            obs_tensor = torch.FloatTensor(np.array(obs_batch)).to(self.device)

            # é€‰æ‹©åŠ¨ä½œ(ç¡®å®šæ€§)
            actions_tensor, _, _ = self.mappo.select_action(
                obs_tensor, deterministic=True
            )

            # è½¬æ¢ä¸ºç¯å¢ƒæ ¼å¼
            env_actions = {}
            for i in range(self.num_agents):
                env_actions[f'agent_{i}'] = {
                    'lane': actions_tensor['lane'][i].cpu().item(),
                    'direction': actions_tensor['direction'][i].cpu().item(),
                    'motion': actions_tensor['motion'][i].cpu().numpy()
                }

            # æ­¥è¿›
            obs_dict, reward_dict, terminated_dict, truncated_dict, info_dict = self.env.step(env_actions)

            # ç»Ÿè®¡æ–¹å‘å˜åŒ–ï¼ˆå•å‘æ¨¡å¼ç†è®ºä¸Šä¸åº”è¯¥å˜åŒ–ï¼‰
            for i, agv in enumerate(self.env.agvs):
                if agv.moving_forward != prev_directions[i]:
                    direction_changes[i] += 1
                prev_directions[i] = agv.moving_forward

            reward_batch = np.array([
                reward_dict[f'agent_{i}'] for i in range(self.num_agents)
            ])
            episode_reward += reward_batch.mean()
            episode_length += 1

            done = terminated_dict['__all__'] or truncated_dict['__all__']

            if render:
                self.env.render()

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        info = info_dict.get('agent_0', {})

        return {
            'reward': episode_reward,
            'length': episode_length,
            'tasks_completed': info.get('completed_tasks', 0),
            'collisions': info.get('collisions', 0),
            'direction_changes': np.mean(direction_changes)
        }

    def evaluate(self, num_episodes: int = 100, render: bool = False):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("\n" + "=" * 60)
        print("ğŸ¯ å¼€å§‹è¯„ä¼°ï¼šæ°´å¹³å•å‘ (h_uni)")
        print("=" * 60)
        print(f"è¯„ä¼°è½®æ•°: {num_episodes}")
        print(f"AGVæ•°é‡: {self.num_agents}")
        print(f"åŒå‘è·¯ç”±: ç¦ç”¨ï¼ˆå•å‘æ¨¡å¼ï¼‰")
        print("=" * 60 + "\n")

        for episode in tqdm(range(num_episodes), desc="Evaluating"):
            result = self.evaluate_episode(render=render)

            self.eval_results['episode_rewards'].append(result['reward'])
            self.eval_results['episode_lengths'].append(result['length'])
            self.eval_results['tasks_completed'].append(result['tasks_completed'])
            self.eval_results['collisions'].append(result['collisions'])
            self.eval_results['direction_changes'].append(result['direction_changes'])

        # æ‰“å°ç»Ÿè®¡
        self.print_statistics()

        # å¯è§†åŒ–ç»“æœ
        self.visualize_results()

        # ä¿å­˜ç»“æœ
        self.save_results()

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°ç»“æœç»Ÿè®¡ - æ°´å¹³å•å‘ (h_uni)")
        print("=" * 60)

        for key, values in self.eval_results.items():
            if values:
                mean_val = np.mean(values)
                std_val = np.std(values)
                min_val = np.min(values)
                max_val = np.max(values)

                display_name = key.replace('_', ' ').title()
                print(f"\n{display_name}:")
                print(f"  Mean: {mean_val:.2f}")
                print(f"  Std:  {std_val:.2f}")
                print(f"  Min:  {min_val:.2f}")
                print(f"  Max:  {max_val:.2f}")

        # ç‰¹åˆ«å¼ºè°ƒå…³é”®æŒ‡æ ‡
        tasks = self.eval_results['tasks_completed']
        if tasks:
            print(f"\n{'=' * 60}")
            print(f"â­ å…³é”®æŒ‡æ ‡ - å•å‘è·¯ç”±é™åˆ¶")
            print(f"{'=' * 60}")
            print(f"  å¹³å‡ä»»åŠ¡å®Œæˆæ•°: {np.mean(tasks):.2f}")
            print(f"  ä»»åŠ¡å®Œæˆç‡: {sum(1 for t in tasks if t > 0) / len(tasks) * 100:.1f}%")
            print(f"  âš ï¸  æ³¨æ„ï¼šAGVåªèƒ½å‰è¿›ï¼Œæ— æ³•åé€€é¿éšœ")
            print(f"{'=' * 60}\n")

    def visualize_results(self):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœ"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Evaluation Results - æ°´å¹³å•å‘ (h_uni)',
                     fontsize=16, fontweight='bold')

        # 1. Episode Rewards
        ax = axes[0, 0]
        ax.plot(self.eval_results['episode_rewards'], alpha=0.6)
        ax.axhline(np.mean(self.eval_results['episode_rewards']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.set_title('Episode Rewards')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. Tasks Completed
        ax = axes[0, 1]
        ax.plot(self.eval_results['tasks_completed'], alpha=0.6,
                color='green', marker='o')
        ax.axhline(np.mean(self.eval_results['tasks_completed']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Tasks')
        ax.set_title('Tasks Completed')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 3. Direction Changes (åº”è¯¥æ¥è¿‘0)
        ax = axes[0, 2]
        ax.plot(self.eval_results['direction_changes'], alpha=0.6,
                color='purple', marker='s')
        ax.axhline(np.mean(self.eval_results['direction_changes']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Direction Changes')
        ax.set_title('Direction Changes (Should be ~0)')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 4. Episode Lengths
        ax = axes[1, 0]
        ax.plot(self.eval_results['episode_lengths'], alpha=0.6, color='orange')
        ax.axhline(np.mean(self.eval_results['episode_lengths']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Length (steps)')
        ax.set_title('Episode Lengths')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 5. Collisions
        ax = axes[1, 1]
        ax.plot(self.eval_results['collisions'], alpha=0.6, color='red')
        ax.axhline(np.mean(self.eval_results['collisions']),
                   color='darkred', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Collisions')
        ax.set_title('Collision Count')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 6. Task Distribution
        ax = axes[1, 2]
        ax.hist(self.eval_results['tasks_completed'], bins=20,
                alpha=0.7, color='skyblue', edgecolor='black')
        ax.axvline(np.mean(self.eval_results['tasks_completed']),
                   color='r', linestyle='--', linewidth=2, label='Mean')
        ax.set_xlabel('Tasks Completed')
        ax.set_ylabel('Frequency')
        ax.set_title('Task Completion Distribution')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()

        save_path = os.path.join(train_config.LOG_DIR, 'evaluation_results_h_uni.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")

        plt.show()

    def save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        results = {
            'experiment': 'h_uni',
            'description': 'æ°´å¹³å¸ƒå±€ + å•å‘è·¯ç”±',
            'statistics': {
                key: {
                    'mean': float(np.mean(values)) if len(values) > 0 else 0.0,
                    'std': float(np.std(values)) if len(values) > 0 else 0.0,
                    'min': float(np.min(values)) if len(values) > 0 else 0.0,
                    'max': float(np.max(values)) if len(values) > 0 else 0.0
                }
                for key, values in self.eval_results.items()
            },
            'raw_data': {
                key: [float(v) for v in values]
                for key, values in self.eval_results.items()
            },
            'config': {
                'num_agents': self.num_agents,
                'bidirectional': False,  # âœ¨ å•å‘
                'num_lanes': env_config.NUM_HORIZONTAL_LANES,
            }
        }

        save_path = os.path.join(train_config.LOG_DIR, 'evaluation_results_h_uni.json')
        with open(save_path, 'w') as f:
            json.dump(results, f, indent=2)

        print(f"âœ… è¯„ä¼°æ•°æ®å·²ä¿å­˜: {save_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Evaluate h_uni Model')
    parser.add_argument(
        '--checkpoint',
        type=str,
        default='./data/checkpoints_h_uni/mappo_final_1000ep.pt',
        help='Path to model checkpoint'
    )
    parser.add_argument(
        '--episodes',
        type=int,
        default=100,
        help='Number of evaluation episodes'
    )
    parser.add_argument(
        '--render',
        action='store_true',
        help='Render environment'
    )

    args = parser.parse_args()

    # è¿è¡Œè¯„ä¼°
    evaluator = Evaluator(args.checkpoint)
    evaluator.evaluate(num_episodes=args.episodes, render=args.render)


if __name__ == "__main__":
    main()