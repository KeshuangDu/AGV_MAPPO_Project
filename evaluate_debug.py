"""
è¯„ä¼°ç¨‹åº - å¸¦è¯¦ç»†è°ƒè¯•ä¿¡æ¯
å¯ä»¥æŸ¥çœ‹æ¨¡å‹çš„è¡Œä¸ºå’Œå†³ç­–è¿‡ç¨‹

è¿è¡Œæ–¹æ³•ï¼š
    python evaluate_debug.py --checkpoint ./data/checkpoints_quick/mappo_final_100ep.pt --episodes 10 --verbose
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import json

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.env_config import env_config
from config.train_config import train_config
from config.model_config import model_config
from environment.port_env import PortEnvironment
from models.actor_critic import ActorCritic
from algorithm.mappo import MAPPO


class DebugEvaluator:
    """è°ƒè¯•è¯„ä¼°å™¨ç±»"""

    def __init__(self, checkpoint_path: str):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.device = torch.device(
            'cuda' if torch.cuda.is_available() and train_config.USE_CUDA
            else 'cpu'
        )
        print(f"Using device: {self.device}")

        # æ‰“å°å½“å‰ä½¿ç”¨çš„é…ç½®
        print("\n" + "="*60)
        print("ğŸ“‹ å½“å‰é…ç½®")
        print("="*60)
        print(f"  - ä»»åŠ¡ç®¡ç†å™¨: {env_config.USE_TASK_MANAGER}")
        print(f"  - å¥–åŠ±ç±»å‹: {env_config.REWARD_TYPE}")
        print(f"  - ä»»åŠ¡å®Œæˆå¥–åŠ±: {env_config.REWARD_WEIGHTS['task_completion']}")
        print(f"  - ç¢°æ’æƒ©ç½š: {env_config.REWARD_WEIGHTS['collision']}")
        print(f"  - åˆ°è¾¾é˜ˆå€¼: {env_config.ARRIVAL_THRESHOLD}ç±³")
        print("="*60 + "\n")

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = PortEnvironment(env_config)
        self.num_agents = env_config.NUM_AGVS

        # è®¡ç®—è§‚å¯Ÿç»´åº¦
        self.obs_dim = 7 + 5*4 + 6 + env_config.NUM_HORIZONTAL_LANES

        # åˆå§‹åŒ–æ¨¡å‹
        self.actor_critic = ActorCritic(
            obs_dim=self.obs_dim,
            actor_hidden_dims=model_config.ACTOR_HIDDEN_DIMS,
            critic_hidden_dims=model_config.CRITIC_HIDDEN_DIMS,
            num_lanes=env_config.NUM_HORIZONTAL_LANES,
            num_directions=2,
            use_centralized_critic=model_config.USE_CENTRALIZED_CRITIC
        )

        # åˆå§‹åŒ–MAPPO
        self.mappo = MAPPO(
            actor_critic=self.actor_critic,
            num_agents=self.num_agents,
            device=self.device
        )

        # åŠ è½½æ¨¡å‹
        if os.path.exists(checkpoint_path):
            self.mappo.load(checkpoint_path)
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {checkpoint_path}")
        else:
            print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹ {checkpoint_path}")
            print("\nå¯ç”¨çš„æ£€æŸ¥ç‚¹ï¼š")
            checkpoint_dir = os.path.dirname(checkpoint_path)
            if os.path.exists(checkpoint_dir):
                checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]
                for cp in checkpoints:
                    print(f"  - {os.path.join(checkpoint_dir, cp)}")
            sys.exit(1)

        # è¯„ä¼°ç»“æœ
        self.eval_results = {
            'episode_rewards': [],
            'episode_lengths': [],
            'tasks_completed': [],
            'collisions': [],
            'direction_changes': []
        }

    def flatten_observation(self, obs_dict: dict) -> np.ndarray:
        """å±•å¹³è§‚å¯Ÿ"""
        obs_list = []
        obs_list.append(obs_dict['own_state'])
        obs_list.append(obs_dict['nearby_agvs'].flatten())
        obs_list.append(obs_dict['task_info'])
        obs_list.append(obs_dict['path_occupancy'])
        return np.concatenate(obs_list, axis=0)

    def evaluate_episode(self, render: bool = False, verbose: bool = False) -> dict:
        """è¯„ä¼°å•ä¸ªepisodeï¼Œverbose=Trueæ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯"""
        obs_dict, _ = self.env.reset()
        episode_reward = 0
        episode_length = 0
        done = False

        # è®°å½•AGVæ–¹å‘å˜åŒ–
        direction_changes = [0] * self.num_agents
        prev_directions = [agv.moving_forward for agv in self.env.agvs]

        # è®°å½•ä»»åŠ¡å®Œæˆæƒ…å†µ
        initial_tasks = len(self.env.tasks)
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ¬ Episode å¼€å§‹")
            print(f"{'='*60}")
            print(f"åˆå§‹ä»»åŠ¡æ•°: {initial_tasks}")
            print(f"AGVæ•°é‡: {self.num_agents}")
            
            # æ‰“å°åˆå§‹AGVçŠ¶æ€
            print(f"\nğŸ“ åˆå§‹AGVä½ç½®:")
            for i, agv in enumerate(self.env.agvs):
                print(f"  AGV{i}: pos=({agv.position[0]:.1f}, {agv.position[1]:.1f})")
            
            # æ‰“å°åˆå§‹ä»»åŠ¡
            print(f"\nğŸ“‹ åˆå§‹ä»»åŠ¡åˆ—è¡¨:")
            for task in self.env.tasks[:3]:  # åªæ‰“å°å‰3ä¸ª
                print(f"  Task{task.id}: {task.type}, "
                      f"pickup={task.pickup_location}, "
                      f"delivery={task.delivery_location}")
            if len(self.env.tasks) > 3:
                print(f"  ... (è¿˜æœ‰ {len(self.env.tasks)-3} ä¸ªä»»åŠ¡)")

        step = 0
        while not done:
            # æ”¶é›†è§‚å¯Ÿ
            obs_batch = []
            for i in range(self.num_agents):
                obs = self.flatten_observation(obs_dict[f'agent_{i}'])
                obs_batch.append(obs)

            obs_tensor = torch.FloatTensor(np.array(obs_batch)).to(self.device)

            # é€‰æ‹©åŠ¨ä½œ(ç¡®å®šæ€§)
            actions_tensor, _, _ = self.mappo.select_action(
                obs_tensor, deterministic=True
            )

            # è½¬æ¢ä¸ºç¯å¢ƒæ ¼å¼
            env_actions = {}
            for i in range(self.num_agents):
                env_actions[f'agent_{i}'] = {
                    'lane': actions_tensor['lane'][i].cpu().item(),
                    'direction': actions_tensor['direction'][i].cpu().item(),
                    'motion': actions_tensor['motion'][i].cpu().numpy()
                }

            # æ­¥è¿›
            obs_dict, reward_dict, terminated_dict, truncated_dict, info_dict = self.env.step(env_actions)

            # ç»Ÿè®¡æ–¹å‘å˜åŒ–
            for i, agv in enumerate(self.env.agvs):
                if agv.moving_forward != prev_directions[i]:
                    direction_changes[i] += 1
                prev_directions[i] = agv.moving_forward

            reward_batch = np.array([
                reward_dict[f'agent_{i}'] for i in range(self.num_agents)
            ])
            episode_reward += reward_batch.mean()
            episode_length += 1
            step += 1

            # è¯¦ç»†è¾“å‡ºï¼ˆæ¯100æ­¥æˆ–ä»»åŠ¡å®Œæˆæ—¶ï¼‰
            if verbose and (step % 100 == 0 or len(self.env.completed_tasks) > 0):
                completed_now = len(self.env.completed_tasks)
                if step % 100 == 0 or completed_now > 0:
                    print(f"\nâ±ï¸  Step {step}")
                    print(f"  å®Œæˆä»»åŠ¡: {completed_now}/{initial_tasks}")
                    print(f"  å¾…å®Œæˆä»»åŠ¡: {len(self.env.tasks)}")
                    print(f"  å½“å‰å¥–åŠ±: {episode_reward:.2f}")
                    
                    # æ‰“å°æ¯ä¸ªAGVçš„çŠ¶æ€
                    for i, agv in enumerate(self.env.agvs):
                        has_task = agv.current_task is not None
                        if has_task:
                            if not agv.has_container:
                                target = agv.current_task['pickup_location']
                                dist = np.linalg.norm(agv.position - target)
                                phase = "â†’pickup"
                            else:
                                target = agv.current_task['delivery_location']
                                dist = np.linalg.norm(agv.position - target)
                                phase = "â†’delivery"
                            
                            # æ˜¾ç¤ºåŠ¨ä½œ
                            action = env_actions[f'agent_{i}']
                            print(f"  AGV{i}: pos=({agv.position[0]:.1f},{agv.position[1]:.1f}), "
                                  f"{phase}, dist={dist:.1f}m, "
                                  f"lane={action['lane']}, dir={action['direction']}, "
                                  f"accel={action['motion'][0]:.2f}")
                        else:
                            print(f"  AGV{i}: pos=({agv.position[0]:.1f},{agv.position[1]:.1f}), idle")

            done = terminated_dict['__all__'] or truncated_dict['__all__']

            if render:
                self.env.render()

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        info = info_dict.get('agent_0', {})
        completed_tasks = info.get('completed_tasks', 0)

        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ Episode ç»“æŸ")
            print(f"{'='*60}")
            print(f"è¿è¡Œæ­¥æ•°: {episode_length}")
            print(f"å®Œæˆä»»åŠ¡æ•°: {completed_tasks}")
            print(f"æ€»å¥–åŠ±: {episode_reward:.2f}")
            print(f"ç¢°æ’æ¬¡æ•°: {info.get('collisions', 0)}")
            print(f"å¹³å‡æ–¹å‘å˜åŒ–: {np.mean(direction_changes):.2f}")
            
            if completed_tasks == 0:
                print(f"\nâš ï¸  è­¦å‘Šï¼šæ²¡æœ‰å®Œæˆä»»ä½•ä»»åŠ¡ï¼")
                print(f"   å¯èƒ½åŸå› ï¼š")
                print(f"   1. æ¨¡å‹è®­ç»ƒä¸è¶³ï¼ˆéœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°ï¼‰")
                print(f"   2. æ¨¡å‹è¿˜æ²¡å­¦ä¼šå¯¼èˆªåˆ°ç›®æ ‡")
                print(f"   3. å¥–åŠ±è®¾ç½®å¯èƒ½éœ€è¦è°ƒæ•´")
            else:
                completion_rate = (completed_tasks / initial_tasks) * 100
                print(f"\nâœ… å®Œæˆç‡: {completion_rate:.1f}%")
            print(f"{'='*60}\n")

        return {
            'reward': episode_reward,
            'length': episode_length,
            'tasks_completed': completed_tasks,
            'collisions': info.get('collisions', 0),
            'direction_changes': np.mean(direction_changes)
        }

    def evaluate(self, num_episodes: int = 100, render: bool = False, verbose: bool = False):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("\n" + "="*60)
        print("ğŸ¯ å¼€å§‹è¯„ä¼°")
        print("="*60)
        print(f"è¯„ä¼°è½®æ•°: {num_episodes}")
        print(f"AGVæ•°é‡: {self.num_agents}")
        print(f"è¯¦ç»†è¾“å‡º: {verbose}")
        print("="*60 + "\n")

        for episode in tqdm(range(num_episodes), desc="Evaluating"):
            # å‰3ä¸ªepisodeä½¿ç”¨è¯¦ç»†è¾“å‡º
            verbose_this_episode = verbose or (episode < 3)

            result = self.evaluate_episode(
                render=render,
                verbose=verbose_this_episode
            )

            self.eval_results['episode_rewards'].append(result['reward'])
            self.eval_results['episode_lengths'].append(result['length'])
            self.eval_results['tasks_completed'].append(result['tasks_completed'])
            self.eval_results['collisions'].append(result['collisions'])
            self.eval_results['direction_changes'].append(result['direction_changes'])

        # è®¡ç®—ç»Ÿè®¡é‡
        self.print_statistics()

        # å¯è§†åŒ–ç»“æœ
        self.visualize_results()

        # ä¿å­˜ç»“æœ
        self.save_results()

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœç»Ÿè®¡")
        print("="*60)

        for key, values in self.eval_results.items():
            if values:
                mean_val = np.mean(values)
                std_val = np.std(values)
                min_val = np.min(values)
                max_val = np.max(values)

                print(f"\n{key.replace('_', ' ').title()}:")
                print(f"  Mean: {mean_val:.2f}")
                print(f"  Std:  {std_val:.2f}")
                print(f"  Min:  {min_val:.2f}")
                print(f"  Max:  {max_val:.2f}")

        # ç‰¹åˆ«å¼ºè°ƒä»»åŠ¡å®Œæˆæƒ…å†µ
        tasks_completed = self.eval_results['tasks_completed']
        if tasks_completed:
            total_tasks = sum(tasks_completed)
            print(f"\n{'='*60}")
            print(f"â­ å…³é”®æŒ‡æ ‡ â­")
            print(f"{'='*60}")
            print(f"  æ€»å®Œæˆä»»åŠ¡æ•°: {total_tasks}")
            print(f"  å¹³å‡æ¯è½®å®Œæˆ: {np.mean(tasks_completed):.2f}")

            if total_tasks == 0:
                print(f"\nâŒ è­¦å‘Šï¼šæ²¡æœ‰å®Œæˆä»»ä½•ä»»åŠ¡ï¼")
                print(f"   è¿™è¯´æ˜æ¨¡å‹è¿˜æ²¡æœ‰å­¦ä¼šä»»åŠ¡å®Œæˆç­–ç•¥")
                print(f"\nğŸ’¡ å»ºè®®ï¼š")
                print(f"   1. è®­ç»ƒæ›´å¤šè½®æ¬¡ï¼ˆè‡³å°‘1000è½®ï¼‰")
                print(f"   2. æ£€æŸ¥å¥–åŠ±è®¾ç½®æ˜¯å¦åˆç†")
                print(f"   3. è€ƒè™‘ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ ")
                print(f"   4. å¢åŠ å¯†é›†å¥–åŠ±çš„å¼•å¯¼æ€§")
            else:
                completed_episodes = sum(1 for t in tasks_completed if t > 0)
                completion_rate = (completed_episodes / len(tasks_completed)) * 100
                print(f"  æœ‰ä»»åŠ¡å®Œæˆçš„è½®æ•°: {completed_episodes}/{len(tasks_completed)}")
                print(f"  è½®æ¬¡å®Œæˆç‡: {completion_rate:.1f}%")
                
                if np.mean(tasks_completed) < 1.0:
                    print(f"\nâš ï¸  ä»»åŠ¡å®Œæˆæ•°è¾ƒä½")
                    print(f"   å»ºè®®ç»§ç»­è®­ç»ƒä»¥æé«˜æ€§èƒ½")
                else:
                    print(f"\nâœ… æ¨¡å‹å·²ç»å­¦ä¼šå®Œæˆä»»åŠ¡ï¼")

        print("\n" + "="*60)

    def visualize_results(self):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœ"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('MAPPO Evaluation Results - Debug Mode',
                     fontsize=16, fontweight='bold')

        # 1. Episode Rewards
        ax = axes[0, 0]
        ax.plot(self.eval_results['episode_rewards'], alpha=0.6)
        ax.axhline(np.mean(self.eval_results['episode_rewards']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.set_title('Episode Rewards')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. Episode Lengths
        ax = axes[0, 1]
        ax.plot(self.eval_results['episode_lengths'], alpha=0.6, color='green')
        ax.axhline(np.mean(self.eval_results['episode_lengths']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Length (steps)')
        ax.set_title('Episode Lengths')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 3. Tasks Completed  âœ¨ é‡ç‚¹å…³æ³¨
        ax = axes[0, 2]
        ax.plot(self.eval_results['tasks_completed'], alpha=0.6, color='orange', marker='o')
        ax.axhline(np.mean(self.eval_results['tasks_completed']),
                   color='r', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Tasks')
        ax.set_title('Tasks Completed (KEY METRIC)')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # å¦‚æœå…¨æ˜¯0ï¼Œæ·»åŠ è­¦å‘Šæ–‡æœ¬
        if sum(self.eval_results['tasks_completed']) == 0:
            ax.text(0.5, 0.5, 'âš ï¸ No Tasks Completed\nModel Needs More Training',
                    horizontalalignment='center',
                    verticalalignment='center',
                    transform=ax.transAxes,
                    fontsize=12,
                    color='red',
                    bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))

        # 4. Collisions
        ax = axes[1, 0]
        ax.plot(self.eval_results['collisions'], alpha=0.6, color='red')
        ax.axhline(np.mean(self.eval_results['collisions']),
                   color='darkred', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Collisions')
        ax.set_title('Collision Count')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 5. Direction Changes
        ax = axes[1, 1]
        ax.plot(self.eval_results['direction_changes'], alpha=0.6, color='purple')
        ax.axhline(np.mean(self.eval_results['direction_changes']),
                   color='darkviolet', linestyle='--', label='Mean')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Direction Changes')
        ax.set_title('Bidirectional Routing Usage')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 6. Task Completion Distribution
        ax = axes[1, 2]
        ax.hist(self.eval_results['tasks_completed'], bins=20,
                alpha=0.7, color='skyblue', edgecolor='black')
        ax.axvline(np.mean(self.eval_results['tasks_completed']),
                   color='r', linestyle='--', linewidth=2, label='Mean')
        ax.set_xlabel('Tasks Completed')
        ax.set_ylabel('Frequency')
        ax.set_title('Task Completion Distribution')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()

        save_path = os.path.join(train_config.LOG_DIR, 'evaluation_results_debug.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")

        plt.show()

    def save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        results = {
            'statistics': {
                key: {
                    'mean': float(np.mean(values)) if len(values) > 0 else 0.0,
                    'std': float(np.std(values)) if len(values) > 0 else 0.0,
                    'min': float(np.min(values)) if len(values) > 0 else 0.0,
                    'max': float(np.max(values)) if len(values) > 0 else 0.0
                }
                for key, values in self.eval_results.items()
            },
            'raw_data': {
                key: [float(v) for v in values]
                for key, values in self.eval_results.items()
            },
            'config': {
                'num_agents': self.num_agents,
                'bidirectional': env_config.BIDIRECTIONAL,
                'num_lanes': env_config.NUM_HORIZONTAL_LANES,
                'use_task_manager': env_config.USE_TASK_MANAGER,
                'reward_type': env_config.REWARD_TYPE
            }
        }

        save_path = os.path.join(train_config.LOG_DIR, 'evaluation_results_debug.json')
        with open(save_path, 'w') as f:
            json.dump(results, f, indent=2)

        print(f"âœ… è¯„ä¼°æ•°æ®å·²ä¿å­˜: {save_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Evaluate MAPPO Model - Debug Mode')
    parser.add_argument(
        '--checkpoint',
        type=str,
        default='./data/checkpoints_quick/mappo_final_100ep.pt',
        help='Path to model checkpoint'
    )
    parser.add_argument(
        '--episodes',
        type=int,
        default=10,
        help='Number of evaluation episodes'
    )
    parser.add_argument(
        '--render',
        action='store_true',
        help='Render environment'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Print detailed information for all episodes'
    )

    args = parser.parse_args()

    # æ£€æŸ¥æ£€æŸ¥ç‚¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.checkpoint):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹ {args.checkpoint}")
        print("\nå¯ç”¨çš„æ£€æŸ¥ç‚¹ï¼š")
        checkpoint_dir = os.path.dirname(args.checkpoint)
        if os.path.exists(checkpoint_dir):
            checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]
            for cp in checkpoints:
                print(f"  - {os.path.join(checkpoint_dir, cp)}")
        return

    # è¿è¡Œè¯„ä¼°
    evaluator = DebugEvaluator(args.checkpoint)
    evaluator.evaluate(
        num_episodes=args.episodes,
        render=args.render,
        verbose=args.verbose
    )


if __name__ == "__main__":
    main()
